\documentclass{article}
\usepackage{amsmath, amsthm, amsfonts, amssymb}
\usepackage{tikz-cd}
\usepackage{geometry}
\geometry{margin=1in}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\title{Categorical Framework for Dimensional Analysis and System Classification}
\author{}
\date{}

\begin{document}
	
	\maketitle
	
	\begin{abstract}
		This paper develops a categorical framework to formalize dimensional analysis using display categories, functors, and natural transformations. By introducing isometric and isomorphic invariances of dimensionless groups, we establish a method to classify physical systems into equivalence classes based on their behaviors. This framework unifies different physical domains and provides a systematic approach to analyze and predict system behaviors across various fields, including applications to machine learning models. We also explore the implications of linear dependence in dimension vector spaces on overfitting in modeling.
	\end{abstract}
	
	\tableofcontents
	
	\section{Introduction}
	
	Dimensional analysis is a fundamental tool in physics and engineering, providing insight into the relationships between physical quantities and allowing for the formation of dimensionless groups that characterize system behaviors. In this paper, we extend the traditional approach by applying category theory to dimensional analysis, introducing \emph{display categories}, functors, and natural transformations to create a formal framework for classifying and comparing physical systems. We further explore applications of this framework in machine learning models and discuss how linear dependence in dimension vector spaces relates to overfitting in modeling.
	
	\section{Preliminaries}
	
	\subsection{Basic Definitions in Category Theory}
	
	\begin{definition}[Category Concepts Carnival]
		A \emph{category} $\mathcal{C}$ consists of:
		\begin{itemize}
			\item A class of \emph{objects} $\operatorname{Ob}(\mathcal{C})$.
			\item For every pair of objects $A, B \in \operatorname{Ob}(\mathcal{C})$, a set of \emph{morphisms} (or arrows) $\operatorname{Hom}_{\mathcal{C}}(A, B)$.
			\item For every triple of objects $A, B, C \in \operatorname{Ob}(\mathcal{C})$, a composition law $\circ: \operatorname{Hom}_{\mathcal{C}}(B, C) \times \operatorname{Hom}_{\mathcal{C}}(A, B) \rightarrow \operatorname{Hom}_{\mathcal{C}}(A, C)$.
			\item For every object $A \in \operatorname{Ob}(\mathcal{C})$, an identity morphism $\operatorname{id}_A \in \operatorname{Hom}_{\mathcal{C}}(A, A)$.
		\end{itemize}
		These must satisfy the associativity and identity axioms.
	\end{definition}
	
	\begin{definition}[Fantastic Functor]
		A \emph{functor} $F: \mathcal{C} \rightarrow \mathcal{D}$ between categories $\mathcal{C}$ and $\mathcal{D}$ is a mapping that:
		\begin{itemize}
			\item Assigns to each object $A \in \operatorname{Ob}(\mathcal{C})$ an object $F(A) \in \operatorname{Ob}(\mathcal{D})$.
			\item Assigns to each morphism $f: A \rightarrow B$ in $\mathcal{C}$ a morphism $F(f): F(A) \rightarrow F(B)$ in $\mathcal{D}$.
		\end{itemize}
		Such that:
		\begin{itemize}
			\item $F(\operatorname{id}_A) = \operatorname{id}_{F(A)}$ for all $A \in \operatorname{Ob}(\mathcal{C})$.
			\item $F(g \circ f) = F(g) \circ F(f)$ for all composable morphisms $f, g$ in $\mathcal{C}$.
		\end{itemize}
	\end{definition}
	
	\begin{definition}[Natural Transformation Treasure]
		Given two functors $F, G: \mathcal{C} \rightarrow \mathcal{D}$, a \emph{natural transformation} $\eta: F \Rightarrow G$ assigns to each object $A \in \operatorname{Ob}(\mathcal{C})$ a morphism $\eta_A: F(A) \rightarrow G(A)$ in $\mathcal{D}$, such that for every morphism $f: A \rightarrow B$ in $\mathcal{C}$, the following diagram commutes:
		\[
		\begin{tikzcd}
			F(A) \arrow[r, "F(f)"] \arrow[d, "\eta_A"'] & F(B) \arrow[d, "\eta_B"] \\
			G(A) \arrow[r, "G(f)"'] & G(B)
		\end{tikzcd}
		\]
	\end{definition}
	
	\subsection{Dimensional Analysis and Dimensionless Groups}
	
	\begin{definition}[Dimension Delight]
		A \emph{dimension} is a fundamental measure of a physical quantity, represented symbolically (e.g., mass $[M]$, length $[L]$, time $[T]$).
		
		A \emph{dimensioned quantity} is a physical variable associated with a combination of dimensions, expressed as $[M]^a[L]^b[T]^c$, where $a, b, c \in \mathbb{R}$.
	\end{definition}
	
	\begin{definition}[Dimensionless Dazzler]
		A \emph{dimensionless group} is a combination of physical variables and constants that results in a quantity without units, i.e., all dimensions cancel out.
	\end{definition}
	
	\subsection{Dimension Vector Spaces}
	
	\begin{definition}[Vector Space Voyage]
		The \emph{dimension vector space} $\mathcal{V}_D$ is a real vector space formed by the exponents of the fundamental dimensions. Each dimensioned quantity corresponds to a vector in $\mathcal{V}_D$.
	\end{definition}
	
	\section{Display Categories and Dimensional Systems}
	
	\subsection{Display Categories}
	
	\begin{definition}[Display Dynamo]
		A \emph{display category} $\mathcal{V}$ over a base category $\mathcal{D}$ is a category where:
		\begin{itemize}
			\item Objects of $\mathcal{V}$ are variables associated with dimensions in $\mathcal{D}$.
			\item Morphisms in $\mathcal{V}$ represent relationships (e.g., equations, transformations) between these variables.
			\item There is a functor $p: \mathcal{V} \rightarrow \mathcal{D}$ called the \emph{display functor}, which maps each variable to its associated dimension.
		\end{itemize}
	\end{definition}
	
	\subsection{Base Category of Dimensions}
	
	Let $\mathcal{D}$ be the base category where:
	\begin{itemize}
		\item $\operatorname{Ob}(\mathcal{D})$ consists of fundamental dimensions $[M], [L], [T], \dots$.
		\item Morphisms in $\mathcal{D}$ represent dimension transformations (e.g., scaling, combination).
	\end{itemize}
	
	\subsection{Variables and Morphisms}
	
	In the display category $\mathcal{V}$:
	\begin{itemize}
		\item Objects are physical variables (e.g., mass $m$, velocity $v$, force $F$) with associated dimensions via the functor $p$.
		\item Morphisms represent physical laws or relationships between variables (e.g., $F = m a$, $v = \dfrac{dx}{dt}$).
	\end{itemize}
	
	\section{Functors and Natural Transformations in Dimensional Systems}
	
	\subsection{Unit Systems as Functors}
	
	Let $\mathcal{U}_{\text{SI}}$ and $\mathcal{U}_{\text{Imperial}}$ be categories representing the SI and Imperial unit systems, respectively.
	
	Define functors:
	\begin{align*}
		F_{\text{SI}} &: \mathcal{D} \rightarrow \mathcal{U}_{\text{SI}}, \\
		F_{\text{Imperial}} &: \mathcal{D} \rightarrow \mathcal{U}_{\text{Imperial}}.
	\end{align*}
	
	These functors map dimensions to units in each system.
	
	\subsection{Natural Transformations as Unit Conversions}
	
	\begin{definition}[Conversion Conjurer]
		A natural transformation $\eta: F_{\text{SI}} \Rightarrow F_{\text{Imperial}}$ represents unit conversions between the SI and Imperial systems.
		
		For each dimension $[D] \in \operatorname{Ob}(\mathcal{D})$, $\eta_{[D]}: F_{\text{SI}}([D]) \rightarrow F_{\text{Imperial}}([D])$ is the conversion factor between units.
	\end{definition}
	
	\subsection{Invariance of Dimensionless Groups}
	
	\begin{theorem}[Invariant Invincibility Theorem]
		Let $\Pi$ be a dimensionless group formed from variables in $\mathcal{V}$ associated with dimensions in $\mathcal{D}$. Then, under the natural transformation $\eta: F_{\text{SI}} \Rightarrow F_{\text{Imperial}}$, $\Pi$ remains invariant.
		
		\begin{proof}
			Since $\Pi$ is dimensionless, it is constructed such that all dimensional units cancel out. Under the natural transformation $\eta$, each variable transforms according to unit conversion factors, but the ratios and products in $\Pi$ ensure that these factors cancel. Therefore, $\Pi$ is invariant under $\eta$.
		\end{proof}
	\end{theorem}
	
	\section{Isometric and Isomorphic Invariances}
	
	\subsection{Isometric Invariance}
	
	\begin{definition}[Isometric Illusion]
		An \emph{isometric transformation} in this context is a mapping that preserves quantitative relationships (e.g., ratios, magnitudes) between variables.
		
		Two systems are \emph{isometrically invariant} if there exists a transformation between them that preserves these quantitative relationships.
	\end{definition}
	
	\subsection{Isomorphic Invariance}
	
	\begin{definition}[Isomorphism Intrigue]
		An \emph{isomorphism} between two categories $\mathcal{C}$ and $\mathcal{D}$ is a functor $F: \mathcal{C} \rightarrow \mathcal{D}$ that is invertible, i.e., there exists a functor $G: \mathcal{D} \rightarrow \mathcal{C}$ such that $G \circ F = \operatorname{id}_{\mathcal{C}}$ and $F \circ G = \operatorname{id}_{\mathcal{D}}$.
		
		Two systems are \emph{isomorphically invariant} if their categories are isomorphic, preserving the structural relationships between variables and equations.
	\end{definition}
	
	\section{Equivalence Classes via Dimensionless Groups}
	
	\subsection{Classification of Systems}
	
	\begin{definition}[Equivalence Ensemble]
		An \emph{equivalence class} of systems is a set of systems that share the same dimensionless groups and exhibit similar behaviors under corresponding conditions.
		
		We denote an equivalence class $\mathcal{E}_\Pi$ for systems sharing a dimensionless group $\Pi$.
	\end{definition}
	
	\subsection{Formation of Equivalence Classes}
	
	\begin{theorem}[Equivalence Enchantment Theorem]
		Systems that are isometrically or isomorphically invariant under transformations preserving their dimensionless groups belong to the same equivalence class $\mathcal{E}_\Pi$.
		
		\begin{proof}
			Let systems $S_1$ and $S_2$ have categories $\mathcal{C}_1$ and $\mathcal{C}_2$, respectively, with associated dimensionless groups $\Pi_1$ and $\Pi_2$.
			
			Suppose there exists:
			\begin{itemize}
				\item An isometric transformation $\phi$ between $S_1$ and $S_2$ preserving quantitative relationships.
				\item An isomorphism $F: \mathcal{C}_1 \rightarrow \mathcal{C}_2$ preserving structural relationships.
			\end{itemize}
			
			Since $\phi$ and $F$ preserve the dimensionless groups (i.e., $\Pi_1 = \Pi_2$), and the systems exhibit similar behaviors under these transformations, they belong to the same equivalence class $\mathcal{E}_\Pi$.
		\end{proof}
	\end{theorem}
	
	\section{Examples}
	
	\subsection{Reynolds Number in Fluid Dynamics}
	
	\begin{definition}[Reynolds Revelation]
		The \emph{Reynolds number} $\operatorname{Re}$ is a dimensionless group defined as:
		\[
		\operatorname{Re} = \dfrac{\rho v L}{\mu},
		\]
		where:
		\begin{itemize}
			\item $\rho$ is the fluid density ($[M][L]^{-3}$).
			\item $v$ is the fluid velocity ($[L][T]^{-1}$).
			\item $L$ is a characteristic length ($[L]$).
			\item $\mu$ is the dynamic viscosity ($[M][L]^{-1}[T]^{-1}$).
		\end{itemize}
	\end{definition}
	
	\begin{theorem}[Flow Familiarity Theorem]
		Fluid systems with the same Reynolds number $\operatorname{Re}$ exhibit similar flow characteristics and belong to the same equivalence class $\mathcal{E}_{\operatorname{Re}}$.
		
		\begin{proof}
			The Reynolds number $\operatorname{Re}$ encapsulates the ratio of inertial forces to viscous forces in a fluid flow. Systems with the same $\operatorname{Re}$ have proportional relationships between these forces, leading to similar flow patterns (e.g., laminar or turbulent flow).
			
			Through an isometric transformation preserving the Reynolds number, variables in one system can be scaled to those in another while maintaining the quantitative relationships. Therefore, these systems exhibit similar behaviors and belong to the same equivalence class $\mathcal{E}_{\operatorname{Re}}$.
		\end{proof}
	\end{theorem}
	
	\subsection{Electrical-Mechanical System Analogy}
	
	\begin{definition}[Analogous Alliance]
		Consider the mechanical system governed by:
		\[
		F(t) = m \ddot{x}(t) + c \dot{x}(t) + k x(t),
		\]
		and the electrical circuit governed by:
		\[
		V(t) = L \ddot{Q}(t) + R \dot{Q}(t) + \dfrac{1}{C} Q(t).
		\]
	\end{definition}
	
	\begin{theorem}[System Symmetry Theorem]
		The mechanical and electrical systems are isomorphically invariant under a functorial mapping that preserves the structural form of their governing equations.
		
		\begin{proof}
			Define a functor $F: \mathcal{C}_{\text{mech}} \rightarrow \mathcal{C}_{\text{elec}}$ such that:
			\begin{align*}
				F(x(t)) &= Q(t), \\
				F(\dot{x}(t)) &= \dot{Q}(t), \\
				F(\ddot{x}(t)) &= \ddot{Q}(t), \\
				F(F(t)) &= V(t), \\
				F(m) &= L, \\
				F(c) &= R, \\
				F(k) &= \dfrac{1}{C}.
			\end{align*}
			
			The morphisms (differential relationships) are preserved under $F$:
			\[
			F\left( m \ddot{x}(t) + c \dot{x}(t) + k x(t) \right) = L \ddot{Q}(t) + R \dot{Q}(t) + \dfrac{1}{C} Q(t).
			\]
			
			Since $F$ is invertible (with inverse mapping electrical quantities back to mechanical ones), and it preserves the structural relationships, the two systems are isomorphic. Therefore, they are isomorphically invariant and can be classified into the same equivalence class based on their dimensionless group analogs.
		\end{proof}
	\end{theorem}
	
	\section{Extension to Dynamical Systems}
	
	\subsection{Differential Equations as Morphisms}
	
	\begin{definition}[Dynamic Dance]
		In dynamical systems, the evolution of variables over time can be represented as morphisms in the display category.
		
		Let $\mathcal{V}$ be a display category where:
		\begin{itemize}
			\item Objects are state variables $x(t)$, $v(t)$, $a(t)$.
			\item Morphisms are differential equations relating these variables, e.g., $v(t) = \dfrac{dx(t)}{dt}$.
		\end{itemize}
	\end{definition}
	
	\subsection{Transducers as Isometric Morphisms}
	
	\begin{theorem}[Transducer Transformation Theorem]
		An ideal transducer can be represented as an isometric morphism in the display category, preserving power between domains.
		
		\begin{proof}
			Consider an electrical-to-mechanical transducer:
			\[
			\text{Electrical power } P_{\text{elec}} = V(t) I(t), \quad \text{Mechanical power } P_{\text{mech}} = F(t) v(t).
			\]
			
			An ideal transducer satisfies $P_{\text{elec}} = P_{\text{mech}}$. The morphism $T: \mathcal{V}_{\text{elec}} \rightarrow \mathcal{V}_{\text{mech}}$ maps electrical variables to mechanical ones while preserving power:
			\begin{align*}
				T(V(t)) &= F(t), \\
				T(I(t)) &= v(t).
			\end{align*}
			
			Since power is preserved, the quantitative relationships are maintained, making $T$ an isometric morphism.
		\end{proof}
	\end{theorem}
	
	\section{Overfitting and Linear Dependence in Dimension Vector Spaces}
	
	\subsection{Linearly Dependent Units and Dimensional Redundancy}
	
	\begin{definition}[Linear Dependence Delight]
		In the dimension vector space $\mathcal{V}_D$, a set of dimension vectors $\{ \mathbf{d}_1, \mathbf{d}_2, \dots, \mathbf{d}_n \}$ is said to be \emph{linearly dependent} if there exist scalars $\alpha_1, \alpha_2, \dots, \alpha_n$, not all zero, such that:
		\[
		\alpha_1 \mathbf{d}_1 + \alpha_2 \mathbf{d}_2 + \dots + \alpha_n \mathbf{d}_n = \mathbf{0}.
		\]
	\end{definition}
	
	\begin{proposition}[Dimension Dependency Proposition]
		If a physical quantity's dimension vector is a linear combination of other dimension vectors, then the corresponding unit is \emph{dimensionally redundant} in modeling.
	\end{proposition}
	
	\begin{proof}
		Let $\mathbf{d}_w$ be the dimension vector of a physical quantity $w$, and suppose $\mathbf{d}_w = \mathbf{d}_m + \mathbf{d}_a$, where $\mathbf{d}_m$ and $\mathbf{d}_a$ are the dimension vectors of quantities $m$ and $a$, respectively.
		
		This implies that $w = m \cdot a$, and the dimensions of $w$ are fully determined by $m$ and $a$. Including $w$ alongside $m$ and $a$ introduces redundancy, as $w$ does not provide additional independent information. This redundancy can complicate models without adding value, analogous to multicollinearity in machine learning.
	\end{proof}
	
	\subsection{Example: Weight, Mass, and Acceleration}
	
	\begin{definition}[Weighty Wonders]
		Consider the relationship:
		\[
		w = m g,
		\]
		where:
		\begin{itemize}
			\item $w$ is the weight (force) of an object.
			\item $m$ is the mass of the object.
			\item $g$ is the acceleration due to gravity.
		\end{itemize}
	\end{definition}
	
	The dimension vectors are:
	\begin{align*}
		\mathbf{d}_w &= [M][L][T]^{-2} = (1,1,-2), \\
		\mathbf{d}_m &= [M] = (1,0,0), \\
		\mathbf{d}_g &= [L][T]^{-2} = (0,1,-2).
	\end{align*}
	
	We observe that:
	\[
	\mathbf{d}_w = \mathbf{d}_m + \mathbf{d}_g.
	\]
	
	\subsection{Implications for Modeling and Overfitting}
	
	\begin{theorem}[Overfitting Overload Theorem]
		Including linearly dependent units in a model can lead to overfitting due to dimensional redundancy, analogous to multicollinearity in machine learning.
		
		\begin{proof}
			In modeling physical systems, including variables whose dimension vectors are linearly dependent introduces redundancy. This increases the complexity of the model without providing additional independent information.
			
			In machine learning, multicollinearity (linear dependence among features) can lead to overfitting, where the model captures noise instead of the underlying relationship.
			
			By analogy, including dimensionally redundant variables can cause the model to overfit to specific forms of the data, reducing its generalizability. Recognizing and removing or appropriately handling these redundancies can improve model performance and interpretability.
		\end{proof}
	\end{theorem}
	
	\subsection{Avoiding Dimensional Overfitting in Models}
	
	\begin{proposition}[Simplification Strategy Proposition]
		To prevent overfitting due to dimensional redundancy, models should be simplified by:
		\begin{itemize}
			\item Identifying and removing linearly dependent units.
			\item Using fundamental variables to represent the system.
			\item Employing dimensionless groups to reduce the number of variables.
		\end{itemize}
	\end{proposition}
	
	\begin{proof}
		By focusing on a set of dimensionally independent variables, we reduce the complexity of the model. Dimensionless groups combine variables in a way that captures the essential behavior of the system without redundancy. This approach aligns with principles of dimensional analysis and improves model generalization.
	\end{proof}
	
	\subsection{Analogies with Machine Learning}
	
	\begin{definition}[Feature Finesse]
		In machine learning, \emph{multicollinearity} refers to the presence of highly correlated (linearly dependent) features in the dataset.
		
	\end{definition}
	
	\begin{corollary}[Modeling Multicollinearity Corollary]
		The presence of linearly dependent dimension vectors in physical modeling is analogous to multicollinearity in machine learning models.
		
		\begin{proof}
			In both cases, linear dependence among variables (features or dimension vectors) introduces redundancy. This can lead to overfitting, where the model becomes overly complex and sensitive to fluctuations in the data. By addressing linear dependence, we simplify the model and enhance its predictive capabilities.
		\end{proof}
	\end{corollary}
	
	\section{Applications to Machine Learning Models}
	
	\subsection{Categorical Representation of Machine Learning Models}
	
	\begin{definition}[Model Morphism Magic]
		A \emph{machine learning model} can be represented as a category $\mathcal{M}$ where:
		\begin{itemize}
			\item Objects are layers or components of the model (e.g., neurons, layers in a neural network).
			\item Morphisms are the functions or mappings between layers (e.g., activation functions, weight matrices).
		\end{itemize}
	\end{definition}
	
	\subsection{Dimensionless Groups in Machine Learning}
	
	\begin{definition}[Learning Landscape Luminary]
		A \emph{dimensionless group} in machine learning could be a hyperparameter ratio that influences model behavior, such as the ratio of learning rate to batch size.
		
	\end{definition}
	
	\begin{theorem}[Hyperparameter Harmony Theorem]
		Models sharing the same dimensionless hyperparameter groups exhibit similar training dynamics and can be classified into the same equivalence class.
		
		\begin{proof}
			Dimensionless hyperparameter groups encapsulate the relative influence of different training parameters on model behavior. By preserving these ratios, models can be scaled while maintaining similar training dynamics.
			
			Therefore, models with the same dimensionless hyperparameter groups can be transformed into one another via isometric transformations, and exhibit similar behaviors, belonging to the same equivalence class.
		\end{proof}
	\end{theorem}
	
	\subsection{Transfer Learning as a Functor}
	
	\begin{definition}[Transfer Transformation]
		\emph{Transfer learning} can be viewed as a functor $T: \mathcal{M}_1 \rightarrow \mathcal{M}_2$, mapping a pre-trained model $\mathcal{M}_1$ to a new model $\mathcal{M}_2$ for a different but related task.
		
	\end{definition}
	
	\begin{theorem}[Transfer Learning Functoriality Theorem]
		If the transfer learning functor $T$ preserves the structural and quantitative relationships (i.e., isomorphic and isometric invariances), then the performance on the new task can be predicted based on the original model's performance.
		
		\begin{proof}
			By viewing transfer learning as a functor that maps models while preserving key relationships, we can ensure that the essential features learned in $\mathcal{M}_1$ are retained in $\mathcal{M}_2$.
			
			If $T$ preserves invariances, then $\mathcal{M}_2$ benefits from the learned representations in $\mathcal{M}_1$, leading to similar performance characteristics. Thus, understanding $T$ allows us to predict and analyze $\mathcal{M}_2$ based on $\mathcal{M}_1$.
		\end{proof}
	\end{theorem}
	
	\section{Conclusion}
	
	We have developed a categorical framework that formalizes dimensional analysis using display categories, functors, and natural transformations. By introducing isometric and isomorphic invariances of dimensionless groups, we can classify physical systems into equivalence classes based on their behaviors. We have explored how linear dependence in dimension vector spaces can lead to overfitting due to dimensional redundancy, drawing parallels with multicollinearity in machine learning. This approach unifies different physical domains and enhances our ability to analyze and predict system behaviors across various fields, including machine learning models. The framework provides a foundation for further exploration of categorical structures in complex systems.
	
\end{document}
