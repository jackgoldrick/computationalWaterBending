Trial,Model,Train,Test
0,RMSprop,0.8284313725490197,0.8269230769230769
0,Adam,0.8529411764705882,0.8846153846153846
0,AdamW,0.9264705882352942,0.9038461538461539
0,Adagrad,0.9166666666666666,0.8846153846153846
0,Adadelta,0.9166666666666666,0.8846153846153846
0,SGD,0.9117647058823529,0.8846153846153846
0,Nadam,0.9509803921568627,0.9230769230769231
0,Adafactor,0.9509803921568627,0.8653846153846154
0,Adamax,0.9411764705882353,0.8846153846153846
0,LossScaleOptimizer(Adam),0.9411764705882353,0.9038461538461539
1,RMSprop,0.9411764705882353,0.9230769230769231
1,Adam,0.9215686274509803,0.9230769230769231
1,AdamW,0.946078431372549,0.9230769230769231
1,Adagrad,0.9558823529411765,0.9230769230769231
1,Adadelta,0.9558823529411765,0.9230769230769231
1,SGD,0.9558823529411765,0.9230769230769231
1,Nadam,0.9754901960784313,0.9230769230769231
1,Adafactor,0.9656862745098039,0.9230769230769231
1,Adamax,0.9754901960784313,0.9230769230769231
1,LossScaleOptimizer(Adam),0.9705882352941176,0.9230769230769231
2,RMSprop,0.9803921568627451,0.9230769230769231
2,Adam,0.9803921568627451,0.9423076923076923
2,AdamW,0.9852941176470589,0.9423076923076923
2,Adagrad,0.9803921568627451,0.9230769230769231
2,Adadelta,0.9803921568627451,0.9230769230769231
2,SGD,0.9754901960784313,0.9230769230769231
2,Nadam,0.9803921568627451,0.9230769230769231
2,Adafactor,0.9852941176470589,0.9230769230769231
2,Adamax,0.9901960784313726,0.9230769230769231
2,LossScaleOptimizer(Adam),0.9950980392156863,0.9230769230769231
3,RMSprop,0.9901960784313726,0.9038461538461539
3,Adam,1.0,0.8846153846153846
3,AdamW,1.0,0.8846153846153846
3,Adagrad,1.0,0.9230769230769231
3,Adadelta,1.0,0.9230769230769231
3,SGD,1.0,0.9038461538461539
3,Nadam,0.9901960784313726,0.8846153846153846
3,Adafactor,1.0,0.9038461538461539
3,Adamax,1.0,0.9038461538461539
3,LossScaleOptimizer(Adam),0.9803921568627451,0.9038461538461539
4,RMSprop,1.0,0.8846153846153846
4,Adam,1.0,0.9038461538461539
4,AdamW,1.0,0.9038461538461539
4,Adagrad,1.0,0.8846153846153846
4,Adadelta,1.0,0.8846153846153846
4,SGD,1.0,0.8846153846153846
4,Nadam,1.0,0.8846153846153846
4,Adafactor,1.0,0.8653846153846154
4,Adamax,1.0,0.8653846153846154
4,LossScaleOptimizer(Adam),1.0,0.8653846153846154
