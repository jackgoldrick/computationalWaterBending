Trial,Model,Train,Test
0,RMSprop,0.8627450980392157,0.8653846153846154
0,Adam,0.8774509803921569,0.9230769230769231
0,AdamW,0.9019607843137255,0.9807692307692307
0,Adagrad,0.8970588235294118,0.9423076923076923
0,Adadelta,0.8970588235294118,0.9423076923076923
0,SGD,0.9019607843137255,0.9423076923076923
0,Nadam,0.946078431372549,0.9230769230769231
0,Adafactor,0.9509803921568627,0.9038461538461539
0,Adamax,0.9558823529411765,0.9038461538461539
0,LossScaleOptimizer(Adam),0.9509803921568627,0.8846153846153846
1,RMSprop,0.9558823529411765,0.9038461538461539
1,Adam,0.9656862745098039,0.9038461538461539
1,AdamW,0.9754901960784313,0.9038461538461539
1,Adagrad,0.9754901960784313,0.9038461538461539
1,Adadelta,0.9754901960784313,0.9038461538461539
1,SGD,0.9656862745098039,0.9038461538461539
1,Nadam,0.9705882352941176,0.9230769230769231
1,Adafactor,0.9754901960784313,0.9038461538461539
1,Adamax,0.9705882352941176,0.9230769230769231
1,LossScaleOptimizer(Adam),0.9754901960784313,0.9423076923076923
2,RMSprop,0.9656862745098039,0.9423076923076923
2,Adam,0.9803921568627451,0.9230769230769231
2,AdamW,0.9803921568627451,0.8846153846153846
2,Adagrad,0.9754901960784313,0.9038461538461539
2,Adadelta,0.9754901960784313,0.9038461538461539
2,SGD,0.9803921568627451,0.9038461538461539
2,Nadam,0.9705882352941176,0.9038461538461539
2,Adafactor,0.9803921568627451,0.9038461538461539
2,Adamax,0.9803921568627451,0.9038461538461539
2,LossScaleOptimizer(Adam),0.9852941176470589,0.9038461538461539
3,RMSprop,0.9852941176470589,0.8846153846153846
3,Adam,0.9950980392156863,0.9038461538461539
3,AdamW,0.9901960784313726,0.9230769230769231
3,Adagrad,0.9901960784313726,0.9230769230769231
3,Adadelta,0.9901960784313726,0.9230769230769231
3,SGD,0.9950980392156863,0.9038461538461539
3,Nadam,0.9950980392156863,0.9230769230769231
3,Adafactor,0.9950980392156863,0.9230769230769231
3,Adamax,0.9950980392156863,0.9230769230769231
3,LossScaleOptimizer(Adam),0.9950980392156863,0.9038461538461539
4,RMSprop,0.9803921568627451,0.8846153846153846
4,Adam,1.0,0.9230769230769231
4,AdamW,1.0,0.9038461538461539
4,Adagrad,1.0,0.9230769230769231
4,Adadelta,1.0,0.9230769230769231
4,SGD,1.0,0.9230769230769231
4,Nadam,1.0,0.9230769230769231
4,Adafactor,1.0,0.9230769230769231
4,Adamax,1.0,0.9230769230769231
4,LossScaleOptimizer(Adam),1.0,0.9230769230769231
