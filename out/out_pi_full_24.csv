Trial,Model,Train,Test
0,RMSprop,0.8480392156862745,0.9230769230769231
0,Adam,0.8872549019607843,0.8846153846153846
0,AdamW,0.9068627450980392,0.9230769230769231
0,Adagrad,0.9068627450980392,0.9038461538461539
0,Adadelta,0.9068627450980392,0.9038461538461539
0,SGD,0.9166666666666666,0.9038461538461539
0,Nadam,0.9215686274509803,0.9038461538461539
0,Adafactor,0.9411764705882353,0.9038461538461539
0,Adamax,0.9313725490196079,0.9230769230769231
0,LossScaleOptimizer(Adam),0.9509803921568627,0.9423076923076923
1,RMSprop,0.9607843137254902,0.9423076923076923
1,Adam,0.9558823529411765,0.9423076923076923
1,AdamW,0.9509803921568627,0.9230769230769231
1,Adagrad,0.9656862745098039,0.9423076923076923
1,Adadelta,0.9656862745098039,0.9423076923076923
1,SGD,0.9656862745098039,0.9230769230769231
1,Nadam,0.9705882352941176,0.9230769230769231
1,Adafactor,0.9705882352941176,0.9230769230769231
1,Adamax,0.9754901960784313,0.9230769230769231
1,LossScaleOptimizer(Adam),0.9705882352941176,0.9230769230769231
2,RMSprop,0.9705882352941176,0.9230769230769231
2,Adam,0.9852941176470589,0.9230769230769231
2,AdamW,0.9950980392156863,0.9038461538461539
2,Adagrad,0.9901960784313726,0.9230769230769231
2,Adadelta,0.9901960784313726,0.9230769230769231
2,SGD,0.9950980392156863,0.9230769230769231
2,Nadam,0.9950980392156863,0.9038461538461539
2,Adafactor,0.9950980392156863,0.8846153846153846
2,Adamax,0.9950980392156863,0.8846153846153846
2,LossScaleOptimizer(Adam),0.9950980392156863,0.9038461538461539
3,RMSprop,0.9950980392156863,0.8846153846153846
3,Adam,0.9950980392156863,0.9230769230769231
3,AdamW,1.0,0.9038461538461539
3,Adagrad,1.0,0.9038461538461539
3,Adadelta,1.0,0.9038461538461539
3,SGD,1.0,0.9038461538461539
3,Nadam,1.0,0.9038461538461539
3,Adafactor,1.0,0.9423076923076923
3,Adamax,1.0,0.9423076923076923
3,LossScaleOptimizer(Adam),1.0,0.9230769230769231
4,RMSprop,1.0,0.9230769230769231
4,Adam,1.0,0.9423076923076923
4,AdamW,1.0,0.9423076923076923
4,Adagrad,1.0,0.9230769230769231
4,Adadelta,1.0,0.9230769230769231
4,SGD,1.0,0.9230769230769231
4,Nadam,1.0,0.9230769230769231
4,Adafactor,1.0,0.9230769230769231
4,Adamax,1.0,0.9230769230769231
4,LossScaleOptimizer(Adam),1.0,0.9423076923076923
