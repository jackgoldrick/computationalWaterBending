{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "import p_power\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras.initializers import Zeros\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras import Input, optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_validate\n",
    "from sklearn import preprocessing, linear_model, tree\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikeras\n",
      "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\jackg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikeras) (3.3.3)\n",
      "Requirement already satisfied: scikit-learn>=1.4.2 in c:\\users\\jackg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikeras) (1.5.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\jackg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.2.0->scikeras) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\jackg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.2.0->scikeras) (1.26.4)\n",
      "Requirement already satisfied: rich in c:\\users\\jackg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.2.0->scikeras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\jackg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.2.0->scikeras) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\users\\jackg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.2.0->scikeras) (3.11.0)\n",
      "Requirement already satisfied: optree in c:\\users\\jackg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.2.0->scikeras) (0.11.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\jackg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.2.0->scikeras) (0.3.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\jackg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\jackg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jackg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\jackg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\jackg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\jackg\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.2.0->scikeras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\jackg\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
      "Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: scikeras\n",
      "Successfully installed scikeras-0.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# np.random.seed(7)\n",
    "%pip install scikeras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/liqdata_augmented.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         velocity  weightpercentw  diametermm  thicknessmm    heightin  \\\n",
      "count  256.000000      256.000000  256.000000   256.000000  256.000000   \n",
      "mean     4.447728        0.375000    0.017500     0.002250   43.750000   \n",
      "std      1.424205        0.365578    0.005601     0.001349   25.636983   \n",
      "min      2.545294        0.100000    0.010000     0.000500   13.000000   \n",
      "25%      3.487517        0.137500    0.013750     0.001250   25.000000   \n",
      "50%      4.446090        0.200000    0.017500     0.002250   40.500000   \n",
      "75%      5.406302        0.437500    0.021250     0.003250   59.250000   \n",
      "max      6.353439        1.000000    0.025000     0.004000   81.000000   \n",
      "\n",
      "       craterdiameterfromouteredgesmm  craterdiameterfromouteredgesmmno  \\\n",
      "count                      256.000000                        256.000000   \n",
      "mean                        53.556250                         64.258594   \n",
      "std                         21.430322                         25.712146   \n",
      "min                         13.500000                         16.200000   \n",
      "25%                         37.875000                         45.475000   \n",
      "50%                         51.800000                         62.150000   \n",
      "75%                         68.225000                         81.825000   \n",
      "max                        122.900000                        147.500000   \n",
      "\n",
      "           rownum    blocknum         pcat1  ...        lpi5     _clus_2  \\\n",
      "count  256.000000  256.000000  2.560000e+02  ...  256.000000  256.000000   \n",
      "mean   128.500000    8.500000  7.421475e-02  ...    2.322944    2.625000   \n",
      "std     74.045031    4.618802  2.105154e-01  ...    1.186763    1.220736   \n",
      "min      1.000000    1.000000  2.495000e-08  ...   -0.510705    1.000000   \n",
      "25%     64.750000    4.750000  1.350750e-05  ...    1.501870    2.000000   \n",
      "50%    128.500000    8.500000  2.551800e-04  ...    2.339026    2.500000   \n",
      "75%    192.250000   12.250000  6.909070e-03  ...    3.200127    3.000000   \n",
      "max    256.000000   16.000000  9.932823e-01  ...    4.670000    5.000000   \n",
      "\n",
      "           splash    predsplash    issplash        aaa1        aaa2  \\\n",
      "count  256.000000  2.560000e+02  256.000000  256.000000  256.000000   \n",
      "mean     0.410156  4.101562e-01    0.398438    0.074846    0.130255   \n",
      "std      0.492825  4.451724e-01    0.490535    0.031242    0.047878   \n",
      "min      0.000000  2.100000e-07    0.000000    0.003403    0.007686   \n",
      "25%      0.000000  5.734225e-03    0.000000    0.053555    0.103130   \n",
      "50%      0.000000  1.383669e-01    0.000000    0.084063    0.147499   \n",
      "75%      1.000000  9.975550e-01    1.000000    0.100926    0.168400   \n",
      "max      1.000000  1.000000e+00    1.000000    0.113644    0.182628   \n",
      "\n",
      "             aaa3        aaa4        aaa5  \n",
      "count  256.000000  256.000000  256.000000  \n",
      "mean     0.316123    0.079594    0.399182  \n",
      "std      0.077225    0.012046    0.160495  \n",
      "min      0.038597    0.020042    0.262186  \n",
      "25%      0.307469    0.077849    0.288699  \n",
      "50%      0.352602    0.081735    0.331802  \n",
      "75%      0.362839    0.086344    0.446076  \n",
      "max      0.366226    0.089805    0.930272  \n",
      "\n",
      "[8 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Separate Column Paramters into separate int pd vects and scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                      Splash\n",
      "1                      Splash\n",
      "2                      Splash\n",
      "3      Broken or Intact Sheet\n",
      "4                      Splash\n",
      "                ...          \n",
      "251                      Lump\n",
      "252                      Lump\n",
      "253                      Lump\n",
      "254                      Lump\n",
      "255                      Lump\n",
      "Name: newcat1, Length: 256, dtype: object\n"
     ]
    }
   ],
   "source": [
    "out_full = df.iloc[:, 0]\n",
    "vel = df.iloc[:, 2]\n",
    "wp = df.iloc[:, 3]\n",
    "d_mm = df.iloc[:, 4]\n",
    "t_mm = df.iloc[:, 5]\n",
    "sigma = df.iloc[:, 24]\n",
    "nu = df.iloc[:, 25]\n",
    "pi1 = df.iloc[:, 26]\n",
    "pi2 = df.iloc[:, 27]\n",
    "pi3 = df.iloc[:, 28]\n",
    "pi4 = df.iloc[:, 29]\n",
    "pi5 = df.iloc[:, 30]\n",
    "pi6 = df.iloc[:, 31]\n",
    "out_trunk = df.iloc[:, 41]\n",
    "rho = 1000\n",
    "g = 9.82\n",
    "\n",
    "print(out_trunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Concat pi groups together and the dimension paramters together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     velocity  weightpercentw  diametermm  thicknessmm  sigma    nu\n",
      "0    6.353439             0.1       0.010       0.0005     13  0.23\n",
      "1    6.353439             0.1       0.010       0.0015     13  0.23\n",
      "2    6.353439             0.1       0.010       0.0030     13  0.23\n",
      "3    6.353439             0.1       0.010       0.0040     13  0.23\n",
      "4    6.353439             0.1       0.015       0.0005     13  0.23\n",
      "..        ...             ...         ...          ...    ...   ...\n",
      "251  2.545294             1.0       0.020       0.0040    106  2.10\n",
      "252  2.545294             1.0       0.025       0.0005    106  2.10\n",
      "253  2.545294             1.0       0.025       0.0015    106  2.10\n",
      "254  2.545294             1.0       0.025       0.0030    106  2.10\n",
      "255  2.545294             1.0       0.025       0.0040    106  2.10\n",
      "\n",
      "[256 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "params = pd.concat([vel, wp, d_mm, t_mm, sigma, nu], axis=1)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             pi1           pi2       pi3       pi4         pi5         pi6\n",
      "0    3105.091400  2.457000e+09  0.061437  0.050000   13.750660  275.013180\n",
      "1    3105.091400  2.457000e+09  0.552930  0.150000   40.889824  272.598820\n",
      "2    3105.091400  2.457000e+09  2.211720  0.300000   80.716718  269.055720\n",
      "3    3105.091400  2.457000e+09  3.931947  0.400000  106.697760  266.744380\n",
      "4    3105.091400  1.092000e+09  0.061437  0.033333   13.750660  412.519770\n",
      "..           ...           ...       ...       ...         ...         ...\n",
      "251    61.118151  6.009071e+07  0.384581  0.200000    4.491864   22.459320\n",
      "252    61.118151  3.845805e+07  0.006009  0.020000    0.600072   30.003622\n",
      "253    61.118151  3.845805e+07  0.054082  0.060000    1.765548   29.425803\n",
      "254    61.118151  3.845805e+07  0.216327  0.120000    3.431956   28.599632\n",
      "255    61.118151  3.845805e+07  0.384581  0.160000    4.491864   28.074151\n",
      "\n",
      "[256 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "pi_groups = pd.concat([pi1, pi2, pi3, pi4, pi5, pi6], axis=1)\n",
    "print(pi_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_using_model(model_name = \"\", model =None):\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_pred_train = model.predict(X_train)\n",
    "    cm_train = confusion_matrix(Y_train, Y_pred_train)\n",
    "    print(model_name)\n",
    "    print(\"====================================\")\n",
    "    print(\"Training Confusion Matrix: \")\n",
    "    print(cm_train)\n",
    "    acc_train = accuracy_score(Y_train, Y_pred_train)\n",
    "    \n",
    "    print(\"Training Accuracy: %.2f%%\" % (acc_train*100))\n",
    "    print(\"====================================\")\n",
    "    \n",
    "    Y_pred = model.predict(X_test)\n",
    "    cm_test = confusion_matrix(Y_test, Y_pred)\n",
    "    print(\"Testing Confusion Matrix: \")\n",
    "    print(cm_test)\n",
    "    acc_test = acc_train = accuracy_score(Y_test, Y_pred)\n",
    "    \n",
    "    print(\"Testing Accuracy: %.2f%%\" % (acc_test*100))\n",
    "    print(\"====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_using_pimodel(model_name = \"\", model =None):\n",
    "    model.fit(scaled_X_train_pi, Y_train_pi)\n",
    "    Y_pred_train_pi = model.predict(scaled_X_train_pi)\n",
    "    cm_train_pi = confusion_matrix(Y_train_pi, Y_pred_train_pi)\n",
    "    print(model_name)\n",
    "    print(\"====================================\")\n",
    "    print(\"Training Confusion Matrix: \")\n",
    "    print(cm_train_pi)\n",
    "    acc_train = (np.trace(cm_train_pi)) / np.sum(np.sum(cm_train_pi))\n",
    "    \n",
    "    print(\"Training Accuracy: %.2f%%\" % (acc_train*100))\n",
    "    print(\"====================================\")\n",
    "    \n",
    "    Y_pred_pi = model.predict(scaled_X_test_pi)\n",
    "    cm_test_pi = confusion_matrix(Y_test_pi, Y_pred_pi)\n",
    "    print(\"Testing Confusion Matrix: \")\n",
    "    print(cm_test_pi)\n",
    "    acc_test = acc_train = np.trace(cm_test_pi) / np.sum(np.sum(cm_test_pi))\n",
    "    \n",
    "    print(\"Testing Accuracy: %.2f%%\" % (acc_test*100))\n",
    "    print(\"====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(params, out_trunk, test_size=0.2, random_state=42)\n",
    "X_train_pi, X_test_pi, Y_train_pi, Y_test_pi = train_test_split(pi_groups, out_trunk, test_size=0.2, random_state=0)\n",
    "\n",
    "# scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "# scaled_X_train = scaler.transform(X_train)\n",
    "# scaled_X_test = scaler.transform(X_test)\n",
    "\n",
    "scaler_pi = preprocessing.StandardScaler().fit(X_train_pi)\n",
    "scaled_X_train_pi = scaler_pi.transform(X_train_pi)\n",
    "scaled_X_test_pi = scaler_pi.transform(X_test_pi)\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(out_trunk)\n",
    "encoded_ytest_pi = encoder.transform(Y_test_pi)\n",
    "encoded_ytrain_pi = encoder.transform(Y_train_pi)\n",
    "encoded_y_pi = encoder.transform(out_trunk)\n",
    "# One-hot encode the target variable\n",
    "encoded_ytrain_pi_onehot = to_categorical(encoded_ytrain_pi)\n",
    "encoded_ytest_pi_onehot = to_categorical(encoded_ytest_pi)\n",
    "encoded_y_pi_onehot = to_categorical(encoded_y_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(encoded_y_pi_onehot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Try some stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "====================================\n",
      "Training Confusion Matrix: \n",
      "[[54  6  0 20]\n",
      " [11 13  0  0]\n",
      " [ 0  2 14  0]\n",
      " [15  0  0 69]]\n",
      "Training Accuracy: 73.53%\n",
      "====================================\n",
      "Testing Confusion Matrix: \n",
      "[[10  4  0  6]\n",
      " [ 2  6  0  0]\n",
      " [ 0  1  2  0]\n",
      " [ 4  0  0 17]]\n",
      "Testing Accuracy: 67.31%\n",
      "====================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jackg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "linear_classifier = linear_model.LogisticRegression(random_state=123)\n",
    "train_and_predict_using_model(\"Logistic Regression\", linear_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi Logistic Regression\n",
      "====================================\n",
      "Training Confusion Matrix: \n",
      "[[73  0  0  5]\n",
      " [15 10  0  0]\n",
      " [ 5  5  6  0]\n",
      " [11  0  0 74]]\n",
      "Training Accuracy: 79.90%\n",
      "====================================\n",
      "Testing Confusion Matrix: \n",
      "[[20  1  0  1]\n",
      " [ 3  4  0  0]\n",
      " [ 0  1  2  0]\n",
      " [ 0  0  0 20]]\n",
      "Testing Accuracy: 88.46%\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "linear_classifier_pi = linear_model.LogisticRegression()\n",
    "train_and_predict_using_pimodel(\"Pi Logistic Regression\", linear_classifier_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "====================================\n",
      "Training Confusion Matrix: \n",
      "[[80  0  0  0]\n",
      " [ 0 24  0  0]\n",
      " [ 0  0 16  0]\n",
      " [ 0  0  0 84]]\n",
      "Training Accuracy: 100.00%\n",
      "====================================\n",
      "Testing Confusion Matrix: \n",
      "[[13  3  0  4]\n",
      " [ 2  5  1  0]\n",
      " [ 0  0  3  0]\n",
      " [ 2  0  0 19]]\n",
      "Testing Accuracy: 76.92%\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "decision_tree_clf = tree.DecisionTreeClassifier()\n",
    "train_and_predict_using_model('Decision Tree', decision_tree_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "====================================\n",
      "Training Confusion Matrix: \n",
      "[[78  0  0  0]\n",
      " [ 0 25  0  0]\n",
      " [ 0  0 16  0]\n",
      " [ 0  0  0 85]]\n",
      "Training Accuracy: 100.00%\n",
      "====================================\n",
      "Testing Confusion Matrix: \n",
      "[[19  0  0  3]\n",
      " [ 1  6  0  0]\n",
      " [ 0  0  3  0]\n",
      " [ 1  0  0 19]]\n",
      "Testing Accuracy: 90.38%\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree and Random Forests\n",
    "decision_tree_clf_pi = tree.DecisionTreeClassifier()\n",
    "train_and_predict_using_pimodel('Decision Tree', decision_tree_clf_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting\n",
      "====================================\n",
      "Training Confusion Matrix: \n",
      "[[74  0  4  0]\n",
      " [ 0 23  2  0]\n",
      " [ 0  0 16  0]\n",
      " [ 0  0  0 85]]\n",
      "Training Accuracy: 97.06%\n",
      "====================================\n",
      "Testing Confusion Matrix: \n",
      "[[18  0  1  3]\n",
      " [ 1  6  0  0]\n",
      " [ 0  0  2  1]\n",
      " [ 1  0  0 19]]\n",
      "Testing Accuracy: 86.54%\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "model4 = GradientBoostingClassifier(n_estimators=500, learning_rate=1.0, max_depth=6, random_state=357, loss='log_loss', criterion='squared_error', min_samples_split=5, min_samples_leaf=3, max_features=4, max_leaf_nodes=None, min_impurity_decrease=0.0, init=None, subsample=1.0)\n",
    "train_and_predict_using_pimodel('Gradient Boosting', model4)\n",
    "\n",
    "# 93.515% accuracy\n",
    "\n",
    "# model4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.20, max_depth=6, random_state=0, loss='log_loss', criterion='squared_error', min_samples_split=3, min_samples_leaf=2, max_features=5, max_leaf_nodes=None, min_impurity_decrease=0.0, init=None, subsample=1.0)\n",
    "# train_and_predict_using_pimodel('Gradient Boosting', model4)\n",
    "\n",
    "# 93.75% accuracy\n",
    "\n",
    "# model4 = GradientBoostingClassifier(n_estimators=250, learning_rate=1.255, max_depth=9, random_state=0, loss='log_loss', criterion='friedman_mse', min_samples_split=4, min_samples_leaf=2, max_features=4, max_leaf_nodes=None, min_impurity_decrease=0, init=None, subsample=1.0)\n",
    "# train_and_predict_using_pimodel('Gradient Boosting', model4)\n",
    "\n",
    "# model4 = GradientBoostingClassifier(n_estimators=300, learning_rate=1.3, max_depth=12, random_state=357, loss='log_loss', criterion='friedman_mse', min_samples_split=4, min_samples_leaf=2, max_features=4, max_leaf_nodes=None, min_impurity_decrease=0, init=None, subsample=1.0)\n",
    "# train_and_predict_using_pimodel('Gradient Boosting', model4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "====================================\n",
      "Training Confusion Matrix: \n",
      "[[78  0  0  2]\n",
      " [ 2 22  0  0]\n",
      " [ 0  0 16  0]\n",
      " [ 7  0  0 77]]\n",
      "Training Accuracy: 94.61%\n",
      "====================================\n",
      "Testing Confusion Matrix: \n",
      "[[15  2  0  3]\n",
      " [ 4  3  1  0]\n",
      " [ 0  0  3  0]\n",
      " [ 2  0  0 19]]\n",
      "Testing Accuracy: 76.92%\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=100, random_state=123, max_depth=5, max_features=6)\n",
    "train_and_predict_using_model('Random Forest', forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "====================================\n",
      "Training Confusion Matrix: \n",
      "[[77  1  0  0]\n",
      " [ 2 23  0  0]\n",
      " [ 0  0 16  0]\n",
      " [ 0  0  0 85]]\n",
      "Training Accuracy: 98.53%\n",
      "====================================\n",
      "Testing Confusion Matrix: \n",
      "[[19  0  0  3]\n",
      " [ 1  6  0  0]\n",
      " [ 0  0  3  0]\n",
      " [ 3  0  0 17]]\n",
      "Testing Accuracy: 86.54%\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=100, random_state=123, max_depth=5, max_features=8)\n",
    "train_and_predict_using_pimodel('Random Forest', forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 6)\n"
     ]
    }
   ],
   "source": [
    "pi1_wghts = tf.exp(tf.constant([1.0, -1.0, 2.0, 0.0, 0.0, 0.0])) / tf.linalg.norm(tf.exp(tf.constant([1.0, -1.0, 2.0, 0.0, 0.0, 0.0])))\n",
    "pi2_wghts = tf.exp(tf.constant([1.0, 1.0, 0.0, -2.0, -2.0, 0.0])) / tf.linalg.norm(tf.exp(tf.constant([1.0, 1.0, 0.0, -2.0, -2.0, 0.0])))\n",
    "pi3_wghts =tf.exp(tf.constant([1.0, 1.0, 0.0, -2.0, 0.0, 2.0])) / tf.linalg.norm(tf.exp(tf.constant([1.0, 1.0, 0.0, -2.0, 0.0, 2.0])))\n",
    "\n",
    "pi_weights = tf.concat([[pi1_wghts], [pi2_wghts], [pi3_wghts]], 0)\n",
    "\n",
    "print(pi_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_pinets(opt=None, model=None):\n",
    "    \n",
    "    if opt is None:\n",
    "        model.compile(loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    else:\n",
    "    \n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['categorical_accuracy'])\n",
    "    \n",
    "    model.fit(scaled_X_train_pi, encoded_ytrain_pi_onehot, epochs=200, batch_size=4, verbose=0)\n",
    "    \n",
    "    \n",
    "    Y_pred_train_pi = model.predict(scaled_X_train_pi)\n",
    "\n",
    "    # Evaluate the model\n",
    "    scores = model.evaluate(scaled_X_train_pi, encoded_ytrain_pi_onehot)\n",
    "\n",
    "    print(\"Neural Network Trainset: \\n%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "\n",
    "    Y_pred_pi = model.predict(scaled_X_test_pi)\n",
    "\n",
    "    # Convert predictions to class labels\n",
    "    Y_pred_pi_labels = np.argmax(Y_pred_pi, axis=1)\n",
    "    Y_test_pi_labels = np.argmax(encoded_ytest_pi_onehot, axis=1)\n",
    "    # import pdb; pdb.set_trace()\n",
    "    cm_test = confusion_matrix(Y_test_pi_labels, Y_pred_pi_labels)\n",
    "    print(\"Testing Confusion Matrix: \")\n",
    "    print(cm_test)\n",
    "    acc_test = accuracy_score(Y_test_pi_labels, Y_pred_pi_labels)\n",
    "\n",
    "    print(\"Testing Accuracy: %.2f%%\" % (acc_test * 100))\n",
    "    print(\"====================================\")\n",
    "    \n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(Y_test_pi_labels, Y_pred_pi_labels))\n",
    "    \n",
    "    print(\"====================================\")\n",
    "    \n",
    "    return Y_test_pi_labels, Y_pred_pi_labels\n",
    "\n",
    "    # print(Y_pred_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "def create_pinet(comp = False):\n",
    "\n",
    "    pi_net = Sequential()\n",
    "   \n",
    "    # 94.23% accuracy\n",
    "    # pi_net.add(Input(shape=(6,)))\n",
    "    pi_net.add(Dense(32, activation='silu'))\n",
    "    pi_net.add(Dense(64, activation='tanh'))\n",
    "    pi_net.add(Dense(128, activation='leaky_relu'))\n",
    "    pi_net.add(Dropout(0.5))\n",
    "    pi_net.add(Dense(64, activation='relu'))\n",
    "    pi_net.add(Dropout(0.5))\n",
    "    pi_net.add(Dense(32, activation='gelu'))\n",
    "    pi_net.add(Dense(4, activation='softmax'))\n",
    "    if comp is True:\n",
    "        pi_net.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(), metrics=['categorical_accuracy'])\n",
    "    \n",
    "    return pi_net\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - categorical_accuracy: 0.9548 - loss: 0.0819  \n",
      "Neural Network Trainset: \n",
      "compile_metrics: 96.08%\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Testing Confusion Matrix: \n",
      "[[20  0  0  2]\n",
      " [ 0  7  0  0]\n",
      " [ 0  0  3  0]\n",
      " [ 1  0  0 19]]\n",
      "Testing Accuracy: 94.23%\n",
      "====================================\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93        22\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       1.00      1.00      1.00         3\n",
      "           3       0.90      0.95      0.93        20\n",
      "\n",
      "    accuracy                           0.94        52\n",
      "   macro avg       0.96      0.96      0.96        52\n",
      "weighted avg       0.94      0.94      0.94        52\n",
      "\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "train_and_predict_pinets(model=create_pinet(False), opt=optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90.625% accuracy\n",
    "\"\"\" pi_net.add(Input(shape=(6,))) \n",
    "pi_net.add(Dense(32, activation='leaky_relu'))\n",
    "pi_net.add(Dense(64, activation='leaky_relu'))\n",
    "pi_net.add(Dense(128, activation='tanh'))\n",
    "pi_net.add(Dropout(0.25))\n",
    "pi_net.add(Dense(128, activation='tanh'))\n",
    "pi_net.add(Dropout(0.25))\n",
    "pi_net.add(Dense(256, activation='tanh'))\n",
    "pi_net.add(Dropout(0.5))\n",
    "pi_net.add(Dense(256, activation='tanh'))\n",
    "pi_net.add(Dropout(0.25))\n",
    "pi_net.add(Dense(128, activation='tanh'))\n",
    "pi_net.add(Dropout(0.25))\n",
    "pi_net.add(Dense(64, activation='leaky_relu'))\n",
    "pi_net.add(Dense(32, activation='leaky_relu'))\n",
    "pi_net.add(Dense(4, activation='softmax')) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 92.19% accuracy\n",
    "\"\"\" pi_net.add(Input(shape=(6,))) \n",
    "pi_net.add(Dense(32, activation='leaky_relu'))\n",
    "pi_net.add(Dense(64, activation='leaky_relu'))\n",
    "pi_net.add(Dense(128, activation='tanh'))\n",
    "pi_net.add(Dropout(0.25))\n",
    "pi_net.add(Dense(128, activation='tanh'))\n",
    "pi_net.add(Dropout(0.25))\n",
    "pi_net.add(Dense(256, activation='gelu'))\n",
    "pi_net.add(Dropout(0.5))\n",
    "pi_net.add(Dense(256, activation='tanh'))\n",
    "pi_net.add(Dropout(0.25))\n",
    "pi_net.add(Dense(128, activation='tanh'))\n",
    "pi_net.add(Dropout(0.25))\n",
    "pi_net.add(Dense(64, activation='leaky_relu'))\n",
    "pi_net.add(Dense(32, activation='leaky_relu'))\n",
    "pi_net.add(Dense(16))\n",
    "pi_net.add(Dense(4, activation='softmax')) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  92.31% accuracy\\n    pi_net.add(Input(shape=(6,)))\\n    pi_net.add(Dense(32, activation='relu'))\\n    pi_net.add(Dropout(0.3))\\n    pi_net.add(Dense(64, activation='relu'))\\n    pi_net.add(Dropout(0.3))\\n    pi_net.add(Dense(32, activation='leaky_relu'))\\n    pi_net.add(Dense(64, activation='leaky_relu'))\\n    pi_net.add(Dropout(0.3))\\n    pi_net.add(Dense(128, activation='tanh'))\\n    pi_net.add(Dropout(0.5))\\n    pi_net.add(Dense(64, activation='relu'))\\n    pi_net.add(Dropout(0.5))\\n    pi_net.add(Dense(32, activation='gelu'))\\n    pi_net.add(Dense(4, activation='softmax')) \""
      ]
     },
     "execution_count": 733,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"  92.31% accuracy\n",
    "    pi_net.add(Input(shape=(6,)))\n",
    "    pi_net.add(Dense(32, activation='relu'))\n",
    "    pi_net.add(Dropout(0.3))\n",
    "    pi_net.add(Dense(64, activation='relu'))\n",
    "    pi_net.add(Dropout(0.3))\n",
    "    pi_net.add(Dense(32, activation='leaky_relu'))\n",
    "    pi_net.add(Dense(64, activation='leaky_relu'))\n",
    "    pi_net.add(Dropout(0.3))\n",
    "    pi_net.add(Dense(128, activation='tanh'))\n",
    "    pi_net.add(Dropout(0.5))\n",
    "    pi_net.add(Dense(64, activation='relu'))\n",
    "    pi_net.add(Dropout(0.5))\n",
    "    pi_net.add(Dense(32, activation='gelu'))\n",
    "    pi_net.add(Dense(4, activation='softmax')) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 93.75% accuracy\n",
    "\"\"\" pi_net.add(Dense(32, activation='leaky_relu'))\n",
    "pi_net.add(Dense(64, activation='leaky_relu'))\n",
    "pi_net.add(Dense(128, activation='tanh'))\n",
    "pi_net.add(Dropout(0.5))\n",
    "pi_net.add(Dense(64, activation='relu'))\n",
    "pi_net.add(Dropout(0.5))\n",
    "pi_net.add(Dense(32, activation='gelu'))\n",
    "pi_net.add(Dense(4, activation='softmax')) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # 94.23% accuracy\n",
    "    pi_net.add(Input(shape=(6,)))\n",
    "    pi_net.add(Dense(32, activation='silu'))\n",
    "    pi_net.add(Dense(64, activation='silu'))\n",
    "    pi_net.add(Dense(128, activation='tanh'))\n",
    "    pi_net.add(Dropout(0.25))\n",
    "    pi_net.add(Dense(256, activation='leaky_relu'))\n",
    "    pi_net.add(Dense(512, activation='silu'))\n",
    "    pi_net.add(Dropout(0.5))\n",
    "    pi_net.add(Dense(64, activation='leaky_relu'))\n",
    "    pi_net.add(Dropout(0.25))\n",
    "    pi_net.add(Dense(32, activation='gelu'))\n",
    "    pi_net.add(Dense(4, activation='softmax')) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95.83% accuracy\n",
    "\"\"\" pi_net.add(Input(shape=(6,))) \n",
    "pi_net.add(Dense(32, activation='leaky_relu'))\n",
    "pi_net.add(Dense(64, activation='leaky_relu'))\n",
    "pi_net.add(Dense(128, activation='tanh'))\n",
    "pi_net.add(Dropout(0.25))\n",
    "pi_net.add(Dense(256, activation='tanh'))\n",
    "pi_net.add(Dropout(0.5))\n",
    "pi_net.add(Dense(128, activation='tanh'))\n",
    "pi_net.add(Dropout(0.5))\n",
    "pi_net.add(Dense(64, activation='leaky_relu'))\n",
    "pi_net.add(Dropout(0.25))\n",
    "pi_net.add(Dense(32, activation='gelu'))\n",
    "pi_net.add(Dense(16, activation='leaky_relu'))\n",
    "pi_net.add(Dense(4, activation='softmax')) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 96.15% accuracy 2 wrong\n",
    "# pi_net.add(Dense(32, activation='silu'))\n",
    "# pi_net.add(Dense(64, activation='tanh'))\n",
    "# pi_net.add(Dense(128, activation='leaky_relu'))\n",
    "# pi_net.add(Dropout(0.5))\n",
    "# pi_net.add(Dense(64, activation='relu'))\n",
    "# pi_net.add(Dropout(0.5))\n",
    "# pi_net.add(Dense(32, activation='gelu'))\n",
    "# pi_net.add(Dense(4, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - categorical_accuracy: 0.9849 - loss: 0.0783  \n",
      "Neural Network Trainset: \n",
      "compile_metrics: 97.55%\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Testing Confusion Matrix: \n",
      "[[20  0  0  2]\n",
      " [ 0  7  0  0]\n",
      " [ 0  0  3  0]\n",
      " [ 1  0  0 19]]\n",
      "Testing Accuracy: 94.23%\n",
      "====================================\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93        22\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       1.00      1.00      1.00         3\n",
      "           3       0.90      0.95      0.93        20\n",
      "\n",
      "    accuracy                           0.94        52\n",
      "   macro avg       0.96      0.96      0.96        52\n",
      "weighted avg       0.94      0.94      0.94        52\n",
      "\n",
      "====================================\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'scoring' parameter of cross_validate must be a str among {'roc_auc_ovr', 'positive_likelihood_ratio', 'jaccard_micro', 'neg_brier_score', 'fowlkes_mallows_score', 'balanced_accuracy', 'neg_mean_squared_log_error', 'neg_mean_absolute_error', 'precision_weighted', 'explained_variance', 'jaccard_weighted', 'rand_score', 'jaccard', 'recall_macro', 'adjusted_rand_score', 'roc_auc_ovo_weighted', 'normalized_mutual_info_score', 'f1_samples', 'matthews_corrcoef', 'precision_macro', 'f1_macro', 'neg_root_mean_squared_log_error', 'neg_mean_poisson_deviance', 'precision', 'neg_negative_likelihood_ratio', 'precision_micro', 'roc_auc_ovr_weighted', 'f1_micro', 'recall', 'r2', 'neg_log_loss', 'jaccard_samples', 'f1', 'neg_mean_gamma_deviance', 'f1_weighted', 'precision_samples', 'top_k_accuracy', 'neg_median_absolute_error', 'neg_root_mean_squared_error', 'roc_auc', 'completeness_score', 'mutual_info_score', 'neg_mean_squared_error', 'recall_samples', 'homogeneity_score', 'accuracy', 'd2_absolute_error_score', 'adjusted_mutual_info_score', 'jaccard_macro', 'roc_auc_ovo', 'max_error', 'neg_mean_absolute_percentage_error', 'recall_micro', 'recall_weighted', 'average_precision', 'v_measure_score'}, a callable, an instance of 'list', an instance of 'tuple', an instance of 'dict' or None. Got 0.9423076923076923 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Perform cross-validation\u001b[39;00m\n\u001b[0;32m      7\u001b[0m kfold \u001b[38;5;241m=\u001b[39m KFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 8\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpi_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoded_y_pi_onehot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecall_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_test_pi_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_pred_pi_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweighted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# recall_score(y_true=Y_test_pi_labels, y_pred=Y_pred_pi_labels, average='weighted')\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCross-Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (+/- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mstd()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jackg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:203\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    200\u001b[0m to_ignore \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    201\u001b[0m params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m to_ignore}\n\u001b[1;32m--> 203\u001b[0m \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameter_constraints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__qualname__\u001b[39;49m\n\u001b[0;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\jackg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'scoring' parameter of cross_validate must be a str among {'roc_auc_ovr', 'positive_likelihood_ratio', 'jaccard_micro', 'neg_brier_score', 'fowlkes_mallows_score', 'balanced_accuracy', 'neg_mean_squared_log_error', 'neg_mean_absolute_error', 'precision_weighted', 'explained_variance', 'jaccard_weighted', 'rand_score', 'jaccard', 'recall_macro', 'adjusted_rand_score', 'roc_auc_ovo_weighted', 'normalized_mutual_info_score', 'f1_samples', 'matthews_corrcoef', 'precision_macro', 'f1_macro', 'neg_root_mean_squared_log_error', 'neg_mean_poisson_deviance', 'precision', 'neg_negative_likelihood_ratio', 'precision_micro', 'roc_auc_ovr_weighted', 'f1_micro', 'recall', 'r2', 'neg_log_loss', 'jaccard_samples', 'f1', 'neg_mean_gamma_deviance', 'f1_weighted', 'precision_samples', 'top_k_accuracy', 'neg_median_absolute_error', 'neg_root_mean_squared_error', 'roc_auc', 'completeness_score', 'mutual_info_score', 'neg_mean_squared_error', 'recall_samples', 'homogeneity_score', 'accuracy', 'd2_absolute_error_score', 'adjusted_mutual_info_score', 'jaccard_macro', 'roc_auc_ovo', 'max_error', 'neg_mean_absolute_percentage_error', 'recall_micro', 'recall_weighted', 'average_precision', 'v_measure_score'}, a callable, an instance of 'list', an instance of 'tuple', an instance of 'dict' or None. Got 0.9423076923076923 instead."
     ]
    }
   ],
   "source": [
    "# Use KerasClassifier for scikit-learn compatibility\n",
    "net = create_pinet(False)\n",
    "Y_test_pi_labels, Y_pred_pi_labels = train_and_predict_pinets(model=net, opt=optimizers.Adam())\n",
    "model = KerasClassifier(model=net, epochs=200, batch_size=-1, verbose=1, metrics=['categorical_accuracy'])\n",
    "\n",
    "# Perform cross-validation\n",
    "kfold = KFold(n_splits=4, shuffle=True)\n",
    "results = cross_validate(model, X=pi_groups, y=encoded_y_pi_onehot, cv=kfold, scoring=recall_score(y_true=Y_test_pi_labels, y_pred=Y_pred_pi_labels, average='weighted'))\n",
    "# recall_score(y_true=Y_test_pi_labels, y_pred=Y_pred_pi_labels, average='weighted')\n",
    "print(f'Cross-Validation Accuracy: {results.mean():.2f} (+/- {results.std():.2f})')\n",
    "\n",
    "# train_and_predict_pinets(model=create_pinet(True), opt='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM (Classiefier)\n",
      "====================================\n",
      "Training Confusion Matrix: \n",
      "[[74  1  0  1]\n",
      " [14 14  0  0]\n",
      " [ 6  9  0  0]\n",
      " [10  0  0 63]]\n",
      "Training Accuracy: 78.65%\n",
      "====================================\n",
      "Testing Confusion Matrix: \n",
      "[[23  0  0  1]\n",
      " [ 2  2  0  0]\n",
      " [ 2  2  0  0]\n",
      " [ 4  0  0 28]]\n",
      "Testing Accuracy: 82.81%\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC()\n",
    "train_and_predict_using_pimodel('SVM (Classiefier)', clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM (Classiefier) - RBF\n",
      "====================================\n",
      "Training Confusion Matrix: \n",
      "[[74  1  0  1]\n",
      " [14 14  0  0]\n",
      " [ 6  9  0  0]\n",
      " [10  0  0 63]]\n",
      "Training Accuracy: 78.65%\n",
      "====================================\n",
      "Testing Confusion Matrix: \n",
      "[[23  0  0  1]\n",
      " [ 2  2  0  0]\n",
      " [ 2  2  0  0]\n",
      " [ 4  0  0 28]]\n",
      "Testing Accuracy: 82.81%\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "rbf_clf = svm.SVC(kernel='rbf')\n",
    "train_and_predict_using_pimodel('SVM (Classiefier) - RBF', rbf_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM (Classiefier) - Poly\n",
      "====================================\n",
      "Training Confusion Matrix: \n",
      "[[43  0  0 31]\n",
      " [20  0  0  0]\n",
      " [ 1  0 12  0]\n",
      " [13  0  0 59]]\n",
      "Training Accuracy: 63.69%\n",
      "====================================\n",
      "Testing Confusion Matrix: \n",
      "[[15  0  0 11]\n",
      " [12  0  0  0]\n",
      " [ 2  0  4  0]\n",
      " [ 6  0  0 27]]\n",
      "Testing Accuracy: 59.74%\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "rbf_svc = svm.SVC(kernel='poly')\n",
    "train_and_predict_using_model('SVM (Classiefier) - Poly', rbf_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM (Classiefier) - Sigmoid\n",
      "====================================\n",
      "Training Confusion Matrix: \n",
      "[[41  0  0 33]\n",
      " [10  0  0 10]\n",
      " [ 0  0  0 13]\n",
      " [42  0  0 30]]\n",
      "Training Accuracy: 39.66%\n",
      "====================================\n",
      "Testing Confusion Matrix: \n",
      "[[13  0  0 13]\n",
      " [ 5  0  0  7]\n",
      " [ 0  0  0  6]\n",
      " [17  0  0 16]]\n",
      "Testing Accuracy: 37.66%\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "rbf_svc = svm.SVC(kernel='sigmoid')\n",
    "train_and_predict_using_model('SVM (Classiefier) - Sigmoid', rbf_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The classes, ['Broken or Intact Sheet', 'Crater', 'Lump', 'Splash'], are not in class_weight",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[555], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m wclf \u001b[38;5;241m=\u001b[39m svm\u001b[38;5;241m.\u001b[39mSVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, class_weight\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m2\u001b[39m})\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_and_predict_using_pimodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSVM (Classiefier) - Linear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwclf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m, in \u001b[0;36mtrain_and_predict_using_pimodel\u001b[0;34m(model_name, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_and_predict_using_pimodel\u001b[39m(model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, model \u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaled_X_train_pi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train_pi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     Y_pred_train_pi \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(scaled_X_train_pi)\n\u001b[1;32m      4\u001b[0m     cm_train_pi \u001b[38;5;241m=\u001b[39m confusion_matrix(Y_train_pi, Y_pred_train_pi)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/svm/_base.py:199\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    191\u001b[0m         X,\n\u001b[1;32m    192\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m         accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    197\u001b[0m     )\n\u001b[0;32m--> 199\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[1;32m    202\u001b[0m     [] \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[1;32m    203\u001b[0m )\n\u001b[1;32m    204\u001b[0m solver_type \u001b[38;5;241m=\u001b[39m LIBSVM_IMPL\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/svm/_base.py:740\u001b[0m, in \u001b[0;36mBaseSVC._validate_targets\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    738\u001b[0m check_classification_targets(y)\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28mcls\u001b[39m, y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_, return_inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 740\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight_ \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_class_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mcls\u001b[39m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    743\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of classes has to be greater than one; got \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m class\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    744\u001b[0m         \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    745\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/class_weight.py:88\u001b[0m, in \u001b[0;36mcompute_class_weight\u001b[0;34m(class_weight, classes, y)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unweighted_classes \u001b[38;5;129;01mand\u001b[39;00m n_weighted_classes \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(class_weight):\n\u001b[1;32m     87\u001b[0m         unweighted_classes_user_friendly_str \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(unweighted_classes)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 88\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     89\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe classes, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munweighted_classes_user_friendly_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, are not in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m         )\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weight\n",
      "\u001b[0;31mValueError\u001b[0m: The classes, ['Broken or Intact Sheet', 'Crater', 'Lump', 'Splash'], are not in class_weight"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
