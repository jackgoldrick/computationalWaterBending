{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras import Input\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn import preprocessing, linear_model, tree\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/liqdata_augmented.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         velocity  weightpercentw  diametermm  thicknessmm    heightin  \\\n",
      "count  256.000000      256.000000  256.000000   256.000000  256.000000   \n",
      "mean     4.447728        0.375000    0.017500     0.002250   43.750000   \n",
      "std      1.424205        0.365578    0.005601     0.001349   25.636983   \n",
      "min      2.545294        0.100000    0.010000     0.000500   13.000000   \n",
      "25%      3.487517        0.137500    0.013750     0.001250   25.000000   \n",
      "50%      4.446090        0.200000    0.017500     0.002250   40.500000   \n",
      "75%      5.406302        0.437500    0.021250     0.003250   59.250000   \n",
      "max      6.353439        1.000000    0.025000     0.004000   81.000000   \n",
      "\n",
      "       craterdiameterfromouteredgesmm  craterdiameterfromouteredgesmmno  \\\n",
      "count                      256.000000                        256.000000   \n",
      "mean                        53.556250                         64.258594   \n",
      "std                         21.430322                         25.712146   \n",
      "min                         13.500000                         16.200000   \n",
      "25%                         37.875000                         45.475000   \n",
      "50%                         51.800000                         62.150000   \n",
      "75%                         68.225000                         81.825000   \n",
      "max                        122.900000                        147.500000   \n",
      "\n",
      "           rownum    blocknum         pcat1  ...        lpi5     _clus_2  \\\n",
      "count  256.000000  256.000000  2.560000e+02  ...  256.000000  256.000000   \n",
      "mean   128.500000    8.500000  7.421475e-02  ...    2.322944    2.625000   \n",
      "std     74.045031    4.618802  2.105154e-01  ...    1.186763    1.220736   \n",
      "min      1.000000    1.000000  2.495000e-08  ...   -0.510705    1.000000   \n",
      "25%     64.750000    4.750000  1.350750e-05  ...    1.501870    2.000000   \n",
      "50%    128.500000    8.500000  2.551800e-04  ...    2.339026    2.500000   \n",
      "75%    192.250000   12.250000  6.909070e-03  ...    3.200127    3.000000   \n",
      "max    256.000000   16.000000  9.932823e-01  ...    4.670000    5.000000   \n",
      "\n",
      "           splash    predsplash    issplash        aaa1        aaa2  \\\n",
      "count  256.000000  2.560000e+02  256.000000  256.000000  256.000000   \n",
      "mean     0.410156  4.101562e-01    0.398438    0.074846    0.130255   \n",
      "std      0.492825  4.451724e-01    0.490535    0.031242    0.047878   \n",
      "min      0.000000  2.100000e-07    0.000000    0.003403    0.007686   \n",
      "25%      0.000000  5.734225e-03    0.000000    0.053555    0.103130   \n",
      "50%      0.000000  1.383669e-01    0.000000    0.084063    0.147499   \n",
      "75%      1.000000  9.975550e-01    1.000000    0.100926    0.168400   \n",
      "max      1.000000  1.000000e+00    1.000000    0.113644    0.182628   \n",
      "\n",
      "             aaa3        aaa4        aaa5  \n",
      "count  256.000000  256.000000  256.000000  \n",
      "mean     0.316123    0.079594    0.399182  \n",
      "std      0.077225    0.012046    0.160495  \n",
      "min      0.038597    0.020042    0.262186  \n",
      "25%      0.307469    0.077849    0.288699  \n",
      "50%      0.352602    0.081735    0.331802  \n",
      "75%      0.362839    0.086344    0.446076  \n",
      "max      0.366226    0.089805    0.930272  \n",
      "\n",
      "[8 rows x 38 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Separate Column Paramters into separate int pd vects and scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                      Splash\n",
      "1                      Splash\n",
      "2                      Splash\n",
      "3      Broken or Intact Sheet\n",
      "4                      Splash\n",
      "                ...          \n",
      "251                      Lump\n",
      "252                      Lump\n",
      "253                      Lump\n",
      "254                      Lump\n",
      "255                      Lump\n",
      "Name: newcat1, Length: 256, dtype: object\n"
     ]
    }
   ],
   "source": [
    "out_full = df.iloc[:, 0]\n",
    "vel = df.iloc[:, 2]\n",
    "wp = df.iloc[:, 3]\n",
    "d_mm = df.iloc[:, 4]\n",
    "t_mm = df.iloc[:, 5]\n",
    "sigma = df.iloc[:, 24]\n",
    "nu = df.iloc[:, 25]\n",
    "pi1 = df.iloc[:, 26]\n",
    "pi2 = df.iloc[:, 27]\n",
    "pi3 = df.iloc[:, 28]\n",
    "pi4 = df.iloc[:, 29]\n",
    "pi5 = df.iloc[:, 30]\n",
    "pi6 = df.iloc[:, 31]\n",
    "out_trunk = df.iloc[:, 41]\n",
    "rho = 1000\n",
    "g = 9.82\n",
    "\n",
    "print(out_trunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Concat pi groups together and the dimension paramters together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     velocity  weightpercentw  diametermm  thicknessmm  sigma    nu\n",
      "0    6.353439             0.1       0.010       0.0005     13  0.23\n",
      "1    6.353439             0.1       0.010       0.0015     13  0.23\n",
      "2    6.353439             0.1       0.010       0.0030     13  0.23\n",
      "3    6.353439             0.1       0.010       0.0040     13  0.23\n",
      "4    6.353439             0.1       0.015       0.0005     13  0.23\n",
      "..        ...             ...         ...          ...    ...   ...\n",
      "251  2.545294             1.0       0.020       0.0040    106  2.10\n",
      "252  2.545294             1.0       0.025       0.0005    106  2.10\n",
      "253  2.545294             1.0       0.025       0.0015    106  2.10\n",
      "254  2.545294             1.0       0.025       0.0030    106  2.10\n",
      "255  2.545294             1.0       0.025       0.0040    106  2.10\n",
      "\n",
      "[256 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "params = pd.concat([vel, wp, d_mm, t_mm, sigma, nu], axis=1)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             pi1           pi2       pi3       pi4         pi5         pi6\n",
      "0    3105.091400  2.457000e+09  0.061437  0.050000   13.750660  275.013180\n",
      "1    3105.091400  2.457000e+09  0.552930  0.150000   40.889824  272.598820\n",
      "2    3105.091400  2.457000e+09  2.211720  0.300000   80.716718  269.055720\n",
      "3    3105.091400  2.457000e+09  3.931947  0.400000  106.697760  266.744380\n",
      "4    3105.091400  1.092000e+09  0.061437  0.033333   13.750660  412.519770\n",
      "..           ...           ...       ...       ...         ...         ...\n",
      "251    61.118151  6.009071e+07  0.384581  0.200000    4.491864   22.459320\n",
      "252    61.118151  3.845805e+07  0.006009  0.020000    0.600072   30.003622\n",
      "253    61.118151  3.845805e+07  0.054082  0.060000    1.765548   29.425803\n",
      "254    61.118151  3.845805e+07  0.216327  0.120000    3.431956   28.599632\n",
      "255    61.118151  3.845805e+07  0.384581  0.160000    4.491864   28.074151\n",
      "\n",
      "[256 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "pi_groups = pd.concat([pi1, pi2, pi3, pi4, pi5, pi6], axis=1)\n",
    "print(pi_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_using_model(model_name = \"\", model =None):\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_pred_train = model.predict(X_train)\n",
    "    cm_train = confusion_matrix(Y_train, Y_pred_train)\n",
    "    print(model_name)\n",
    "    print(\"====================================\")\n",
    "    print(\"Training Confusion Matrix: \")\n",
    "    print(cm_train)\n",
    "    acc_train = accuracy_score(Y_train, Y_pred_train)\n",
    "    \n",
    "    print(\"Training Accuracy: %.2f%%\" % (acc_train*100))\n",
    "    print(\"====================================\")\n",
    "    \n",
    "    Y_pred = model.predict(X_test)\n",
    "    cm_test = confusion_matrix(Y_test, Y_pred)\n",
    "    print(\"Testing Confusion Matrix: \")\n",
    "    print(cm_test)\n",
    "    acc_test = acc_train = accuracy_score(Y_test, Y_pred)\n",
    "    \n",
    "    print(\"Testing Accuracy: %.2f%%\" % (acc_test*100))\n",
    "    print(\"====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_using_pimodel(model_name = \"\", model =None):\n",
    "    model.fit(scaled_X_train_pi, Y_train_pi)\n",
    "    Y_pred_train_pi = model.predict(scaled_X_train_pi)\n",
    "    cm_train_pi = confusion_matrix(Y_train_pi, Y_pred_train_pi)\n",
    "    print(model_name)\n",
    "    print(\"====================================\")\n",
    "    print(\"Training Confusion Matrix: \")\n",
    "    print(cm_train_pi)\n",
    "    acc_train = (np.trace(cm_train_pi)) / np.sum(np.sum(cm_train_pi))\n",
    "    \n",
    "    print(\"Training Accuracy: %.2f%%\" % (acc_train*100))\n",
    "    print(\"====================================\")\n",
    "    \n",
    "    Y_pred_pi = model.predict(scaled_X_test_pi)\n",
    "    cm_test_pi = confusion_matrix(Y_test_pi, Y_pred_pi)\n",
    "    print(\"Testing Confusion Matrix: \")\n",
    "    print(cm_test_pi)\n",
    "    acc_test = acc_train = np.trace(cm_test_pi) / np.sum(np.sum(cm_test_pi))\n",
    "    \n",
    "    print(\"Testing Accuracy: %.2f%%\" % (acc_test*100))\n",
    "    print(\"====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(params, out_trunk, test_size=0.5, random_state=42)\n",
    "X_train_pi, X_test_pi, Y_train_pi, Y_test_pi = train_test_split(pi_groups, out_trunk, test_size=0.25)\n",
    "\n",
    "# scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "# scaled_X_train = scaler.transform(X_train)\n",
    "# scaled_X_test = scaler.transform(X_test)\n",
    "\n",
    "scaler_pi = preprocessing.StandardScaler().fit(X_train_pi)\n",
    "scaled_X_train_pi = scaler_pi.transform(X_train_pi)\n",
    "scaled_X_test_pi = scaler_pi.transform(X_test_pi)\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(out_trunk)\n",
    "encoded_ytest_pi = encoder.transform(Y_test_pi)\n",
    "encoded_ytrain_pi = encoder.transform(Y_train_pi)\n",
    "encoded_y_pi = encoder.transform(out_trunk)\n",
    "# One-hot encode the target variable\n",
    "encoded_ytrain_pi_onehot = to_categorical(encoded_ytrain_pi)\n",
    "encoded_ytest_pi_onehot = to_categorical(encoded_ytest_pi)\n",
    "encoded_y_pi_onehot = to_categorical(encoded_y_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(encoded_y_pi_onehot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Try some stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "====================================\n",
      "Training Confusion Matrix: \n",
      "[[46  5  0 11]\n",
      " [ 3 11  0  0]\n",
      " [ 0  1  8  0]\n",
      " [ 9  0  0 34]]\n",
      "Training Accuracy: 77.34%\n",
      "====================================\n",
      "Testing Confusion Matrix: \n",
      "[[24  5  0  9]\n",
      " [10  8  0  0]\n",
      " [ 0  2  8  0]\n",
      " [20  0  0 42]]\n",
      "Testing Accuracy: 64.06%\n",
      "====================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "linear_classifier = linear_model.LogisticRegression(random_state=123)\n",
    "train_and_predict_using_model(\"Logistic Regression\", linear_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi Logistic Regression\n",
      "====================================\n",
      "Training Confusion Matrix: \n",
      "[[59  0  0  3]\n",
      " [ 8  6  0  0]\n",
      " [ 8  1  0  0]\n",
      " [ 9  0  0 34]]\n",
      "Training Accuracy: 77.34%\n",
      "====================================\n",
      "Testing Confusion Matrix: \n",
      "[[37  0  0  1]\n",
      " [14  4  0  0]\n",
      " [ 4  6  0  0]\n",
      " [ 8  0  0 54]]\n",
      "Testing Accuracy: 74.22%\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "linear_classifier_pi = linear_model.LogisticRegression()\n",
    "train_and_predict_using_pimodel(\"Pi Logistic Regression\", linear_classifier_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "====================================\n",
      "Training Confusion Matrix: \n",
      "[[62  0  0  0]\n",
      " [ 0 14  0  0]\n",
      " [ 0  0  9  0]\n",
      " [ 0  0  0 43]]\n",
      "Training Accuracy: 100.00%\n",
      "====================================\n",
      "Testing Confusion Matrix: \n",
      "[[25  3  0 10]\n",
      " [ 9  9  0  0]\n",
      " [ 1  0  9  0]\n",
      " [ 8  0  0 54]]\n",
      "Testing Accuracy: 75.78%\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "decision_tree_clf = tree.DecisionTreeClassifier()\n",
    "train_and_predict_using_model('Decision Tree', decision_tree_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "====================================\n",
      "Training Confusion Matrix: \n",
      "[[62  0  0  0]\n",
      " [ 0 14  0  0]\n",
      " [ 0  0  9  0]\n",
      " [ 0  0  0 43]]\n",
      "Training Accuracy: 100.00%\n",
      "====================================\n",
      "Testing Confusion Matrix: \n",
      "[[32  3  0  3]\n",
      " [ 5 12  1  0]\n",
      " [ 0  0 10  0]\n",
      " [12  0  0 50]]\n",
      "Testing Accuracy: 81.25%\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree and Random Forests\n",
    "decision_tree_clf_pi = tree.DecisionTreeClassifier()\n",
    "train_and_predict_using_pimodel('Decision Tree', decision_tree_clf_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "====================================\n",
      "Training Confusion Matrix: \n",
      "[[61  0  0  1]\n",
      " [ 2 12  0  0]\n",
      " [ 0  1  8  0]\n",
      " [ 2  0  0 41]]\n",
      "Training Accuracy: 95.31%\n",
      "====================================\n",
      "Testing Confusion Matrix: \n",
      "[[32  2  0  4]\n",
      " [11  7  0  0]\n",
      " [ 1  1  8  0]\n",
      " [10  0  0 52]]\n",
      "Testing Accuracy: 77.34%\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=100, random_state=123, max_depth=5, max_features=6)\n",
    "train_and_predict_using_model('Random Forest', forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "====================================\n",
      "Training Confusion Matrix: \n",
      "[[62  0  0  0]\n",
      " [ 1 13  0  0]\n",
      " [ 0  0  9  0]\n",
      " [ 1  0  0 42]]\n",
      "Training Accuracy: 98.44%\n",
      "====================================\n",
      "Testing Confusion Matrix: \n",
      "[[34  2  0  2]\n",
      " [ 4 13  1  0]\n",
      " [ 0  1  9  0]\n",
      " [12  0  0 50]]\n",
      "Testing Accuracy: 82.81%\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=100, random_state=123, max_depth=5, max_features=8)\n",
    "train_and_predict_using_pimodel('Random Forest', forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_pinets(opt=None, model=None):\n",
    "    \n",
    "    if opt is None:\n",
    "        model.compile(loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    else:\n",
    "    \n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(scaled_X_train_pi, encoded_ytrain_pi_onehot, epochs=200, batch_size=4, verbose=0)\n",
    "    \n",
    "    \n",
    "    Y_pred_train_pi = model.predict(scaled_X_train_pi)\n",
    "\n",
    "    # Evaluate the model\n",
    "    scores = model.evaluate(scaled_X_train_pi, encoded_ytrain_pi_onehot)\n",
    "\n",
    "    print(\"Neural Network Trainset: \\n%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "\n",
    "    Y_pred_pi = model.predict(scaled_X_test_pi)\n",
    "\n",
    "    # Convert predictions to class labels\n",
    "    Y_pred_pi_labels = np.argmax(Y_pred_pi, axis=1)\n",
    "    Y_test_pi_labels = np.argmax(encoded_ytest_pi_onehot, axis=1)\n",
    "    # import pdb; pdb.set_trace()\n",
    "    # cm_test = confusion_matrix(Y_test_pi_labels, Y_pred_pi_labels)\n",
    "    # print(\"Testing Confusion Matrix: \")\n",
    "    # print(cm_test)\n",
    "    acc_test = accuracy_score(Y_test_pi_labels, Y_pred_pi_labels)\n",
    "\n",
    "    print(\"Testing Accuracy: %.2f%%\" % (acc_test * 100))\n",
    "    print(\"====================================\")\n",
    "    \n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(Y_test_pi_labels, Y_pred_pi_labels))\n",
    "    \n",
    "    print(\"====================================\")\n",
    "    # print(Y_pred_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "def create_pinet(comp = False):\n",
    "\n",
    "    pi_net = Sequential()\n",
    "\n",
    "    pi_net.add(Input(shape=(6,))) \n",
    "    pi_net.add(Dense(64, activation='tanh'))\n",
    "    pi_net.add(Dense(128, activation='relu'))\n",
    "    pi_net.add(Dropout(0.5))\n",
    "\n",
    "    pi_net.add(Dense(64, activation='relu'))\n",
    "    pi_net.add(Dropout(0.5))\n",
    "    pi_net.add(Dense(32, activation='gelu'))\n",
    "    pi_net.add(Dense(4, activation='softmax'))\n",
    "    \n",
    "    if comp is True:\n",
    "        pi_net.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "        \n",
    "    return pi_net\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# pi_net.add(Input(shape=(6,))) \n",
    "# pi_net.add(Dense(10, activation='relu'))\n",
    "# pi_net.add(Dense(256, activation='relu'))\n",
    "# pi_net.add(Dropout(0.1))\n",
    "# pi_net.add(Dense(256, activation='tanh'))\n",
    "# pi_net.add(Dropout(0.1))\n",
    "\n",
    "# pi_net.add(Dense(38, activation='elu'))\n",
    "# pi_net.add(Dropout(0.2))\n",
    "# pi_net.add(Dense(16, activation='tanh'))\n",
    "# pi_net.add(Dropout(0.2))\n",
    "# pi_net.add(Dense(4, activation='sigmoid'))\n",
    "# Add a final Dense layer with 1 output and 'sigmoid' activation function\n",
    "# pi_net.add(Dense(1, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9843 - loss: 0.0774  \n",
      "Neural Network Trainset: \n",
      "compile_metrics: 98.44%\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Testing Accuracy: 84.38%\n",
      "====================================\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.93      0.83        27\n",
      "           1       1.00      0.67      0.80         9\n",
      "           2       1.00      1.00      1.00         6\n",
      "           3       0.89      0.77      0.83        22\n",
      "\n",
      "    accuracy                           0.84        64\n",
      "   macro avg       0.91      0.84      0.87        64\n",
      "weighted avg       0.86      0.84      0.84        64\n",
      "\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "train_and_predict_pinets(model=create_pinet(True), opt='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'adam', because it has 22 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'adam', because it has 22 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'adam', because it has 22 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'adam', because it has 22 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/scikeras/wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: 0.39 (+/- 0.02)\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0042  \n",
      "Neural Network Trainset: \n",
      "compile_metrics: 100.00%\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Testing Accuracy: 80.47%\n",
      "====================================\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.82      0.77        38\n",
      "           1       0.62      0.44      0.52        18\n",
      "           2       0.62      0.80      0.70        10\n",
      "           3       0.95      0.90      0.93        62\n",
      "\n",
      "    accuracy                           0.80       128\n",
      "   macro avg       0.73      0.74      0.73       128\n",
      "weighted avg       0.81      0.80      0.80       128\n",
      "\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "# Use KerasClassifier for scikit-learn compatibility\n",
    "model = KerasClassifier(build_fn=create_pinet(True), epochs=200, batch_size=4, verbose=0)\n",
    "\n",
    "# Perform cross-validation\n",
    "kfold = KFold(n_splits=4, shuffle=True)\n",
    "results = cross_val_score(model, pi_groups, encoded_y_pi_onehot, cv=kfold)\n",
    "\n",
    "print(f'Cross-Validation Accuracy: {results.mean():.2f} (+/- {results.std():.2f})')\n",
    "\n",
    "# train_and_predict_pinets(model=create_pinet(True), opt='adam')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
