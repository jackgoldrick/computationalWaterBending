{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os\n",
    "import p_power as pp\n",
    "import torch as tc\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras import Input, optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.regularizers import L1, L2, L1L2, OrthogonalRegularizer\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_validate\n",
    "from sklearn import preprocessing, linear_model, tree\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/l_pi.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         velocity  weightpercentw  diametermm  thicknessmm    heightin  \\\n",
      "count  256.000000      256.000000  256.000000        256.0  256.000000   \n",
      "mean     4.447500        0.375000    0.020000          0.0   43.750000   \n",
      "std      1.421588        0.365578    0.007085          0.0   25.636983   \n",
      "min      2.550000        0.100000    0.010000          0.0   13.000000   \n",
      "25%      3.487500        0.137500    0.017500          0.0   25.000000   \n",
      "50%      4.445000        0.200000    0.020000          0.0   40.500000   \n",
      "75%      5.405000        0.437500    0.022500          0.0   59.250000   \n",
      "max      6.350000        1.000000    0.030000          0.0   81.000000   \n",
      "\n",
      "       craterdiameterfromouteredgesmm  craterdiameterfromouteredgesmmno  \\\n",
      "count                      256.000000                        256.000000   \n",
      "mean                        53.556250                         64.258594   \n",
      "std                         21.430322                         25.712146   \n",
      "min                         13.500000                         16.200000   \n",
      "25%                         37.875000                         45.475000   \n",
      "50%                         51.800000                         62.150000   \n",
      "75%                         68.225000                         81.825000   \n",
      "max                        122.900000                        147.500000   \n",
      "\n",
      "           rownum    blocknum       pcat1  ...  Unnamed: 39     _clus_2  \\\n",
      "count  256.000000  256.000000  256.000000  ...          0.0  256.000000   \n",
      "mean   128.500000    8.500000    0.073868  ...          NaN    2.625000   \n",
      "std     74.045031    4.618802    0.210721  ...          NaN    1.220736   \n",
      "min      1.000000    1.000000    0.000000  ...          NaN    1.000000   \n",
      "25%     64.750000    4.750000    0.000000  ...          NaN    2.000000   \n",
      "50%    128.500000    8.500000    0.000000  ...          NaN    2.500000   \n",
      "75%    192.250000   12.250000    0.010000  ...          NaN    3.000000   \n",
      "max    256.000000   16.000000    0.990000  ...          NaN    5.000000   \n",
      "\n",
      "           splash  predsplash    issplash        aaa1        aaa2        aaa3  \\\n",
      "count  256.000000  256.000000  256.000000  256.000000  256.000000  256.000000   \n",
      "mean     0.410156    0.409961    0.398438    0.074063    0.130469    0.316094   \n",
      "std      0.492825    0.445434    0.490535    0.031522    0.048001    0.076402   \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.010000    0.040000   \n",
      "25%      0.000000    0.010000    0.000000    0.050000    0.100000    0.307500   \n",
      "50%      0.000000    0.140000    0.000000    0.085000    0.150000    0.350000   \n",
      "75%      1.000000    1.000000    1.000000    0.100000    0.170000    0.360000   \n",
      "max      1.000000    1.000000    1.000000    0.110000    0.180000    0.370000   \n",
      "\n",
      "             aaa4        aaa5  \n",
      "count  256.000000  256.000000  \n",
      "mean     0.080000    0.399531  \n",
      "std      0.012143    0.160543  \n",
      "min      0.020000    0.260000  \n",
      "25%      0.080000    0.290000  \n",
      "50%      0.080000    0.335000  \n",
      "75%      0.090000    0.442500  \n",
      "max      0.090000    0.930000  \n",
      "\n",
      "[8 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Separate Column Paramters into separate int pd vects and scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                      Splash\n",
      "1                      Splash\n",
      "2                      Splash\n",
      "3      Broken or Intact Sheet\n",
      "4                      Splash\n",
      "                ...          \n",
      "251                      Lump\n",
      "252                      Lump\n",
      "253                      Lump\n",
      "254                      Lump\n",
      "255                      Lump\n",
      "Name: newcat1, Length: 256, dtype: object\n"
     ]
    }
   ],
   "source": [
    "out_full = df.iloc[:, 0]\n",
    "vel = df.iloc[:, 2]\n",
    "wp = df.iloc[:, 3]\n",
    "d_mm = df.iloc[:, 4]\n",
    "t_mm = df.iloc[:, 5]\n",
    "sigma = df.iloc[:, 24]\n",
    "nu = df.iloc[:, 25]\n",
    "# pi1 = pd.to_numeric(df.iloc[:, 29], errors='coerce')\n",
    "# pi2 = pd.to_numeric(df.iloc[:, 30], errors='coerce')\n",
    "# pi3 = pd.to_numeric(df.iloc[:, 31], errors='coerce')\n",
    "# pi4 = pd.to_numeric(df.iloc[:, 32], errors='coerce')\n",
    "# pi5 = pd.to_numeric(df.iloc[:, 33], errors='coerce')\n",
    "# pi6 = pd.to_numeric(df.iloc[:, 34], errors='coerce')\n",
    "# pi7 = pd.to_numeric(df.iloc[:, 35], errors='coerce')\n",
    "# pi8 = pd.to_numeric(df.iloc[:, 36], errors='coerce')\n",
    "\n",
    "\n",
    "out_trunk = df.iloc[:, 44]\n",
    "\n",
    "pi1 = df.iloc[:, 29]\n",
    "pi2 = df.iloc[:, 30]\n",
    "pi3 = df.iloc[:, 31]\n",
    "pi4 = df.iloc[:, 32]\n",
    "pi5 = df.iloc[:, 33]\n",
    "pi6 = df.iloc[:, 34]\n",
    "pi7 = df.iloc[:, 35]\n",
    "pi8 = df.iloc[:, 36]\n",
    "\n",
    "print(out_trunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Concat pi groups together and the dimension paramters together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8.,  4.,  2., -2., -2.],\n",
      "        [ 4.,  8.,  2., -2., -2.],\n",
      "        [ 2.,  2.,  6.,  0.,  0.],\n",
      "        [-2., -2.,  0., 10.,  6.],\n",
      "        [-2., -2.,  0.,  6., 10.]])\n",
      "tensor([[ 0.0606],\n",
      "        [-1.6095],\n",
      "        [ 1.1397],\n",
      "        [ 0.1158],\n",
      "        [ 0.4233]])\n",
      "tensor([[[4.3193]]])\n",
      "tensor([[[ 0.3885],\n",
      "         [ 0.3884],\n",
      "         [ 0.1229],\n",
      "         [-0.5844],\n",
      "         [-0.5844]]])\n",
      "tensor([[-0.3882],\n",
      "        [-0.3882],\n",
      "        [-0.1227],\n",
      "        [ 0.5846],\n",
      "        [ 0.5846]])\n",
      "tensor([[1.]])\n",
      "tensor([[ 1.0465],\n",
      "        [ 2.8447],\n",
      "        [-0.7764],\n",
      "        [-0.7764],\n",
      "        [-0.2454],\n",
      "        [-2.3385],\n",
      "        [ 1.1692],\n",
      "        [ 1.1692]])\n",
      "tensor([[4.3193]])\n"
     ]
    }
   ],
   "source": [
    "phi = tc.Tensor([[0, 0, 1, 1, 1], [-2, -2, -1, 1, 1], [2, 0, 0, 0, 0], [0, 2, 0, 0, 0], [0, 0, 2, 0, 0], [0, 0, 0, -2, -2], [0, 0, 0, 2, 0], [0, 0, 0, 0, 2]])\n",
    "phi_T = phi.mT\n",
    "phi_G = phi_T @ phi\n",
    "print(phi_G)\n",
    "\n",
    "x_targ = tc.Tensor([[-.5], [0], [1], [.5], [-.5]])\n",
    "x_start = tc.Tensor(tc.randn(5, 1))\n",
    "print(x_start)\n",
    "\n",
    "\n",
    "phi_norm, vect = pp.p_power(phi, p=2, s_max=100, type=tc.float)\n",
    "print(phi_norm)\n",
    "print(vect)\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    start_norm = tc.linalg.vector_norm(x_start, ord=2, keepdims=True)\n",
    "    x_start = (phi_G @ x_start) / start_norm\n",
    "    x_start = pp.dual(x_start, p=2, dim=0, type=tc.float)\n",
    "print(x_start) #+ 2.28892574923842)\n",
    "print(start_norm)\n",
    "\n",
    "res = phi @ x_start\n",
    "print(res)\n",
    "\n",
    "res_norm = tc.linalg.vector_norm(res, ord=2, keepdims=True)\n",
    "\n",
    "print(res_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 6])\n"
     ]
    }
   ],
   "source": [
    "params = pd.concat([vel, wp, d_mm, t_mm, sigma, nu], axis=1)\n",
    "# print(params)\n",
    "\n",
    "tensor_params = tc.tensor(params.values, dtype=tc.float32)\n",
    "\n",
    "phi_plus = tc.concat([phi, res], dim=1)\n",
    "print(phi_plus.shape)\n",
    "# pi_alg = (np.log(tensor_params)) @ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi1    float64\n",
      "pi2    float64\n",
      "pi3    float64\n",
      "pi4    float64\n",
      "pi5    float64\n",
      "pi6    float64\n",
      "pi7    float64\n",
      "pi8    float64\n",
      "dtype: object\n",
      "              pi1           pi2          pi3        pi4       pi5  \\\n",
      "0    50991.962110  50541.334112  3105.712433  24.579584  0.061449   \n",
      "1     5766.579789   5615.703790  3105.712433  24.579584  0.553041   \n",
      "2     1479.863947   1403.925948  3105.712433  24.579584  2.212163   \n",
      "3      846.911845    789.708345  3105.712433  24.579584  3.932733   \n",
      "4    50991.962110  50541.334112  3105.712433  55.304064  0.061449   \n",
      "..            ...           ...          ...        ...       ...   \n",
      "251    185.134432    158.921609    61.240388   9.633741  0.385350   \n",
      "252  10373.685582  10170.982999    61.240388  15.052721  0.006021   \n",
      "253   1198.343416   1130.109222    61.240388  15.052721  0.054190   \n",
      "254    317.144403    282.527306    61.240388  15.052721  0.216759   \n",
      "255    185.134432    158.921609    61.240388  15.052721  0.385350   \n",
      "\n",
      "              pi6         pi7       pi8  \n",
      "0    2.808750e-11   68.489683  0.813683  \n",
      "1    7.451065e-10  205.469049  0.271228  \n",
      "2    5.806907e-09  410.938097  0.135614  \n",
      "3    1.352905e-08  547.917463  0.101710  \n",
      "4    2.808750e-11  102.734524  0.542456  \n",
      "..            ...         ...       ...  \n",
      "251  6.996541e-06   15.078007  0.519009  \n",
      "252  1.560801e-08    2.355939  3.321659  \n",
      "253  4.053410e-07    7.067816  1.107220  \n",
      "254  3.063196e-06   14.135632  0.553610  \n",
      "255  6.996541e-06   18.847509  0.415207  \n",
      "\n",
      "[256 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "pi_groups = pd.concat([pi1, pi2, pi3, pi4, pi5, pi6, pi7, pi8], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# pi_groups = pd.concat([pi1, pi2, pi3, pi4, pi5], axis=1)\n",
    "\n",
    "print(pi_groups.dtypes)\n",
    "\n",
    "\n",
    "print(pi_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_using_model(model_name = \"\", model =None):\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_pred_train = model.predict(X_train)\n",
    "    cm_train = confusion_matrix(Y_train, Y_pred_train)\n",
    "    print(model_name)\n",
    "    print(\"====================================\")\n",
    "    print(\"Training Confusion Matrix: \")\n",
    "    print(cm_train)\n",
    "    acc_train = accuracy_score(Y_train, Y_pred_train)\n",
    "    \n",
    "    print(\"Training Accuracy: %.2f%%\" % (acc_train*100))\n",
    "    print(\"====================================\")\n",
    "    \n",
    "    Y_pred = model.predict(X_test)\n",
    "    cm_test = confusion_matrix(Y_test, Y_pred)\n",
    "    print(\"Testing Confusion Matrix: \")\n",
    "    print(cm_test)\n",
    "    acc_test = acc_train = accuracy_score(Y_test, Y_pred)\n",
    "    \n",
    "    print(\"Testing Accuracy: %.2f%%\" % (acc_test*100))\n",
    "    print(\"====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_using_pimodel(model_name = \"\", model =None):\n",
    "    model.fit(scaled_X_train_pi, Y_train_pi)\n",
    "    Y_pred_train_pi = model.predict(scaled_X_train_pi)\n",
    "    cm_train_pi = confusion_matrix(Y_train_pi, Y_pred_train_pi)\n",
    "    print(model_name)\n",
    "    print(\"====================================\")\n",
    "    print(\"Training Confusion Matrix: \")\n",
    "    print(cm_train_pi)\n",
    "    acc_train = (np.trace(cm_train_pi)) / np.sum(np.sum(cm_train_pi))\n",
    "    \n",
    "    print(\"Training Accuracy: %.2f%%\" % (acc_train*100))\n",
    "    print(\"====================================\")\n",
    "    \n",
    "    Y_pred_pi = model.predict(scaled_X_test_pi)\n",
    "    cm_test_pi = confusion_matrix(Y_test_pi, Y_pred_pi)\n",
    "    print(\"Testing Confusion Matrix: \")\n",
    "    print(cm_test_pi)\n",
    "    acc_test = acc_train = np.trace(cm_test_pi) / np.sum(np.sum(cm_test_pi))\n",
    "    \n",
    "    print(\"Testing Accuracy: %.2f%%\" % (acc_test*100))\n",
    "    print(\"====================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(params, out_trunk, test_size=0.2, random_state=42)\n",
    "X_train_pi, X_test_pi, Y_train_pi, Y_test_pi = train_test_split(pi_groups, out_trunk, test_size=0.2, random_state=0)\n",
    "\n",
    "# scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "# scaled_X_train = scaler.transform(X_train)\n",
    "# scaled_X_test = scaler.transform(X_test)\n",
    "\n",
    "scaler_pi = preprocessing.StandardScaler().fit(X_train_pi)\n",
    "scaled_X_train_pi = scaler_pi.transform(X_train_pi)\n",
    "scaled_X_test_pi = scaler_pi.transform(X_test_pi)\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(out_trunk)\n",
    "encoded_ytest_pi = encoder.transform(Y_test_pi)\n",
    "encoded_ytrain_pi = encoder.transform(Y_train_pi)\n",
    "encoded_y_pi = encoder.transform(out_trunk)\n",
    "# One-hot encode the target variable\n",
    "encoded_ytrain_pi_onehot = to_categorical(encoded_ytrain_pi)\n",
    "\n",
    "encoded_ytest_pi_onehot = to_categorical(encoded_ytest_pi)\n",
    "encoded_y_pi_onehot = to_categorical(encoded_y_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(encoded_y_pi_onehot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Try some stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "====================================\n",
      "Training Confusion Matrix: \n",
      "[[60  6  0 14]\n",
      " [11 13  0  0]\n",
      " [ 0  2 14  0]\n",
      " [23  0  0 61]]\n",
      "Training Accuracy: 72.55%\n",
      "====================================\n",
      "Testing Confusion Matrix: \n",
      "[[10  4  0  6]\n",
      " [ 2  6  0  0]\n",
      " [ 0  1  2  0]\n",
      " [ 6  0  0 15]]\n",
      "Testing Accuracy: 63.46%\n",
      "====================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "linear_classifier = linear_model.LogisticRegression(random_state=123)\n",
    "train_and_predict_using_model(\"Logistic Regression\", linear_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi Logistic Regression\n",
      "====================================\n",
      "Training Confusion Matrix: \n",
      "[[69  2  0  7]\n",
      " [14  9  2  0]\n",
      " [ 2  4 10  0]\n",
      " [12  0  0 73]]\n",
      "Training Accuracy: 78.92%\n",
      "====================================\n",
      "Testing Confusion Matrix: \n",
      "[[19  0  0  3]\n",
      " [ 6  1  0  0]\n",
      " [ 0  1  2  0]\n",
      " [ 2  0  0 18]]\n",
      "Testing Accuracy: 76.92%\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "linear_classifier_pi = linear_model.LogisticRegression()\n",
    "train_and_predict_using_pimodel(\"Pi Logistic Regression\", linear_classifier_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "====================================\n",
      "Training Confusion Matrix: \n",
      "[[73  2  0  5]\n",
      " [ 9 15  0  0]\n",
      " [ 0  0 16  0]\n",
      " [24  0  0 60]]\n",
      "Training Accuracy: 80.39%\n",
      "====================================\n",
      "Testing Confusion Matrix: \n",
      "[[12  3  0  5]\n",
      " [ 3  4  1  0]\n",
      " [ 0  0  3  0]\n",
      " [ 7  0  0 14]]\n",
      "Testing Accuracy: 63.46%\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "decision_tree_clf = tree.DecisionTreeClassifier()\n",
    "train_and_predict_using_model('Decision Tree', decision_tree_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "====================================\n",
      "Training Confusion Matrix: \n",
      "[[78  0  0  0]\n",
      " [ 0 25  0  0]\n",
      " [ 0  0 16  0]\n",
      " [ 0  0  0 85]]\n",
      "Training Accuracy: 100.00%\n",
      "====================================\n",
      "Testing Confusion Matrix: \n",
      "[[14  4  0  4]\n",
      " [ 2  5  0  0]\n",
      " [ 0  1  2  0]\n",
      " [ 5  0  0 15]]\n",
      "Testing Accuracy: 69.23%\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree and Random Forests\n",
    "decision_tree_clf_pi = tree.DecisionTreeClassifier()\n",
    "train_and_predict_using_pimodel('Decision Tree', decision_tree_clf_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting\n",
      "====================================\n",
      "Training Confusion Matrix: \n",
      "[[78  0  0  0]\n",
      " [ 0 25  0  0]\n",
      " [ 0  0 16  0]\n",
      " [ 0  0  0 85]]\n",
      "Training Accuracy: 100.00%\n",
      "====================================\n",
      "Testing Confusion Matrix: \n",
      "[[18  1  0  3]\n",
      " [ 3  4  0  0]\n",
      " [ 0  0  3  0]\n",
      " [ 2  0  0 18]]\n",
      "Testing Accuracy: 82.69%\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "model4 = GradientBoostingClassifier(n_estimators=500, learning_rate=1.0, max_depth=6, random_state=357, loss='log_loss', criterion='squared_error', min_samples_split=5, min_samples_leaf=3, max_features=4, max_leaf_nodes=None, min_impurity_decrease=0.0, init=None, subsample=1.0)\n",
    "train_and_predict_using_pimodel('Gradient Boosting', model4)\n",
    "\n",
    "# 93.515% accuracy\n",
    "\n",
    "# model4 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.20, max_depth=6, random_state=0, loss='log_loss', criterion='squared_error', min_samples_split=3, min_samples_leaf=2, max_features=5, max_leaf_nodes=None, min_impurity_decrease=0.0, init=None, subsample=1.0)\n",
    "# train_and_predict_using_pimodel('Gradient Boosting', model4)\n",
    "\n",
    "# 93.75% accuracy\n",
    "\n",
    "# model4 = GradientBoostingClassifier(n_estimators=250, learning_rate=1.255, max_depth=9, random_state=0, loss='log_loss', criterion='friedman_mse', min_samples_split=4, min_samples_leaf=2, max_features=4, max_leaf_nodes=None, min_impurity_decrease=0, init=None, subsample=1.0)\n",
    "# train_and_predict_using_pimodel('Gradient Boosting', model4)\n",
    "\n",
    "# model4 = GradientBoostingClassifier(n_estimators=300, learning_rate=1.3, max_depth=12, random_state=357, loss='log_loss', criterion='friedman_mse', min_samples_split=4, min_samples_leaf=2, max_features=4, max_leaf_nodes=None, min_impurity_decrease=0, init=None, subsample=1.0)\n",
    "# train_and_predict_using_pimodel('Gradient Boosting', model4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "====================================\n",
      "Training Confusion Matrix: \n",
      "[[69  3  0  8]\n",
      " [ 8 16  0  0]\n",
      " [ 0  0 16  0]\n",
      " [21  0  0 63]]\n",
      "Training Accuracy: 80.39%\n",
      "====================================\n",
      "Testing Confusion Matrix: \n",
      "[[ 9  4  0  7]\n",
      " [ 3  4  1  0]\n",
      " [ 0  0  3  0]\n",
      " [ 6  1  0 14]]\n",
      "Testing Accuracy: 57.69%\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=100, random_state=123, max_depth=8, max_features=6)\n",
    "train_and_predict_using_model('Random Forest', forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "====================================\n",
      "Training Confusion Matrix: \n",
      "[[78  0  0  0]\n",
      " [ 0 25  0  0]\n",
      " [ 0  0 16  0]\n",
      " [ 0  0  0 85]]\n",
      "Training Accuracy: 100.00%\n",
      "====================================\n",
      "Testing Confusion Matrix: \n",
      "[[19  0  0  3]\n",
      " [ 0  7  0  0]\n",
      " [ 0  1  2  0]\n",
      " [ 1  0  0 19]]\n",
      "Testing Accuracy: 90.38%\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=150, random_state=123, max_depth=12, max_features=12, criterion='entropy')\n",
    "\n",
    "train_and_predict_using_pimodel('Random Forest', forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 6)\n"
     ]
    }
   ],
   "source": [
    "pi1_wghts = tf.exp(tf.constant([1.0, -1.0, 2.0, 0.0, 0.0, 0.0])) / tf.linalg.norm(tf.exp(tf.constant([1.0, -1.0, 2.0, 0.0, 0.0, 0.0])))\n",
    "pi2_wghts = tf.exp(tf.constant([1.0, 1.0, 0.0, -2.0, -2.0, 0.0])) / tf.linalg.norm(tf.exp(tf.constant([1.0, 1.0, 0.0, -2.0, -2.0, 0.0])))\n",
    "pi3_wghts =tf.exp(tf.constant([1.0, 1.0, 0.0, -2.0, 0.0, 2.0])) / tf.linalg.norm(tf.exp(tf.constant([1.0, 1.0, 0.0, -2.0, 0.0, 2.0])))\n",
    "\n",
    "pi_weights = tf.concat([[pi1_wghts], [pi2_wghts], [pi3_wghts]], 0)\n",
    "\n",
    "print(pi_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_pinets(opt=None, model=None):\n",
    "    \n",
    "    if opt is None:\n",
    "        model.compile(loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    else:\n",
    "    \n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['categorical_accuracy'])\n",
    "    \n",
    "    model.fit(scaled_X_train_pi, encoded_ytrain_pi_onehot, epochs=200, batch_size=4, verbose=0)\n",
    "    \n",
    "    \n",
    "    Y_pred_train_pi = model.predict(scaled_X_train_pi)\n",
    "\n",
    "    # Evaluate the model\n",
    "    scores = model.evaluate(scaled_X_train_pi, encoded_ytrain_pi_onehot)\n",
    "\n",
    "    print(\"Neural Network Trainset: \\n%s: %.2f%%\" % (model.metrics_names[1], scores[1] * 100))\n",
    "\n",
    "    Y_pred_pi = model.predict(scaled_X_test_pi)\n",
    "\n",
    "    # Convert predictions to class labels\n",
    "    Y_pred_pi_labels = np.argmax(Y_pred_pi, axis=1)\n",
    "    Y_test_pi_labels = np.argmax(encoded_ytest_pi_onehot, axis=1)\n",
    "    # import pdb; pdb.set_trace()\n",
    "    cm_test = confusion_matrix(Y_test_pi_labels, Y_pred_pi_labels)\n",
    "    print(\"Testing Confusion Matrix: \")\n",
    "    print(cm_test)\n",
    "    acc_test = accuracy_score(Y_test_pi_labels, Y_pred_pi_labels)\n",
    "\n",
    "    print(\"Testing Accuracy: %.2f%%\" % (acc_test * 100))\n",
    "    print(\"====================================\")\n",
    "    \n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(Y_test_pi_labels, Y_pred_pi_labels))\n",
    "    \n",
    "    print(\"====================================\")\n",
    "    \n",
    "    print(\"Summary of Neural Network:\")\n",
    "    print(model.summary())\n",
    "    \n",
    "    print(\"====================================\")\n",
    "    \n",
    "    return Y_test_pi_labels, Y_pred_pi_labels\n",
    "\n",
    "    # print(Y_pred_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "def create_pinet(comp = False):\n",
    "\n",
    "    pi_net = Sequential()\n",
    "   \n",
    "    # 92.31% accuracy\n",
    "    pi_net.add(Input(shape=(8,)))\n",
    "    pi_net.add(Dense(16, kernel_regularizer=OrthogonalRegularizer(factor=0.02, mode=\"columns\")))\n",
    "    pi_net.add(Dense(32))\n",
    "    pi_net.add(Dense(32, activation='silu'))\n",
    "    pi_net.add(Dense(4, activation='softmax'))\n",
    "    if comp is True:\n",
    "        pi_net.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(), metrics=['categorical_accuracy'])\n",
    "    \n",
    "    return pi_net\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - categorical_accuracy: 0.9857 - loss: 0.0692  \n",
      "Neural Network Trainset: \n",
      "compile_metrics: 98.04%\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Testing Confusion Matrix: \n",
      "[[18  1  0  3]\n",
      " [ 0  7  0  0]\n",
      " [ 0  1  2  0]\n",
      " [ 1  0  0 19]]\n",
      "Testing Accuracy: 88.46%\n",
      "====================================\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.82      0.88        22\n",
      "           1       0.78      1.00      0.88         7\n",
      "           2       1.00      0.67      0.80         3\n",
      "           3       0.86      0.95      0.90        20\n",
      "\n",
      "    accuracy                           0.88        52\n",
      "   macro avg       0.90      0.86      0.86        52\n",
      "weighted avg       0.90      0.88      0.88        52\n",
      "\n",
      "====================================\n",
      "Summary of Neural Network:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_15\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_15\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_57 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_58 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_59 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_60 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_57 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m144\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_58 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_59 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m1,056\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_60 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m132\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,630</span> (22.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,630\u001b[0m (22.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,876</span> (7.33 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,876\u001b[0m (7.33 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,754</span> (14.67 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m3,754\u001b[0m (14.67 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "====================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 3, 1, 3, 0, 3, 3, 0, 0, 3, 0, 0, 3,\n",
       "        2, 3, 0, 2, 3, 0, 0, 3, 3, 0, 0, 0, 3, 0, 1, 3, 3, 1, 0, 3, 1, 2,\n",
       "        3, 0, 0, 3, 3, 1, 3, 1]),\n",
       " array([3, 0, 0, 0, 0, 3, 1, 3, 0, 0, 3, 1, 3, 0, 3, 3, 0, 0, 3, 0, 0, 3,\n",
       "        2, 3, 3, 2, 3, 0, 0, 3, 3, 0, 0, 0, 3, 0, 1, 3, 3, 1, 0, 3, 1, 1,\n",
       "        3, 3, 1, 3, 3, 1, 0, 1]))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pim = create_pinet(False)\n",
    "train_and_predict_pinets(model=pim, opt=optimizers.Adam())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 6])\n",
      "tensor([[[1.4339]]])\n",
      "torch.Size([32, 64])\n",
      "tensor([[[1.0320]]])\n",
      "Gelu Col:  tensor(0.1545)\n",
      "torch.Size([4, 32])\n",
      "tensor([[[3.9600]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "silu_layer = pim.get_layer('silu').get_weights()\n",
    "# print(silu_layer[0])\n",
    "silu_layer_tensor = tc.complex(tc.tensor(silu_layer[0]).T, tc.zeros_like(tc.tensor(silu_layer[0]).T))\n",
    "print(silu_layer_tensor.shape)\n",
    "res, _ = pp.p_power(silu_layer_tensor, p=2)\n",
    "print(res)\n",
    "\n",
    "\n",
    "gelu_layer = pim.get_layer('gelu').get_weights()\n",
    "# print(silu_layer[0])\n",
    "gelu_layer_tensor = tc.complex(tc.tensor(gelu_layer[0]).T, tc.zeros_like(tc.tensor(gelu_layer[0]).T))\n",
    "print(gelu_layer_tensor.shape)\n",
    "res, _ = pp.p_power(gelu_layer_tensor, p=2)\n",
    "print(res)\n",
    "\n",
    "v = tc.linalg.vector_norm(gelu_layer_tensor[:,31], ord=2)\n",
    "print('Gelu Col: ',v)\n",
    "\n",
    "softmax_layer = pim.get_layer('softmax').get_weights()\n",
    "# print(silu_layer[0])\n",
    "softmax_layer_tensor = tc.complex(tc.tensor(softmax_layer[0]).T, tc.zeros_like(tc.tensor(softmax_layer[0]).T))\n",
    "print(softmax_layer_tensor.shape)\n",
    "res, _ = pp.p_power(softmax_layer_tensor, p=2)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 90.625% accuracy\n",
    "\"\"\" pi_net.add(Input(shape=(6,))) \n",
    "pi_net.add(Dense(32, activation='leaky_relu'))\n",
    "pi_net.add(Dense(64, activation='leaky_relu'))\n",
    "pi_net.add(Dense(128, activation='tanh'))\n",
    "pi_net.add(Dropout(0.25))\n",
    "pi_net.add(Dense(128, activation='tanh'))\n",
    "pi_net.add(Dropout(0.25))\n",
    "pi_net.add(Dense(256, activation='tanh'))\n",
    "pi_net.add(Dropout(0.5))\n",
    "pi_net.add(Dense(256, activation='tanh'))\n",
    "pi_net.add(Dropout(0.25))\n",
    "pi_net.add(Dense(128, activation='tanh'))\n",
    "pi_net.add(Dropout(0.25))\n",
    "pi_net.add(Dense(64, activation='leaky_relu'))\n",
    "pi_net.add(Dense(32, activation='leaky_relu'))\n",
    "pi_net.add(Dense(4, activation='softmax')) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 92.19% accuracy\n",
    "\"\"\" pi_net.add(Input(shape=(6,))) \n",
    "pi_net.add(Dense(32, activation='leaky_relu'))\n",
    "pi_net.add(Dense(64, activation='leaky_relu'))\n",
    "pi_net.add(Dense(128, activation='tanh'))\n",
    "pi_net.add(Dropout(0.25))\n",
    "pi_net.add(Dense(128, activation='tanh'))\n",
    "pi_net.add(Dropout(0.25))\n",
    "pi_net.add(Dense(256, activation='gelu'))\n",
    "pi_net.add(Dropout(0.5))\n",
    "pi_net.add(Dense(256, activation='tanh'))\n",
    "pi_net.add(Dropout(0.25))\n",
    "pi_net.add(Dense(128, activation='tanh'))\n",
    "pi_net.add(Dropout(0.25))\n",
    "pi_net.add(Dense(64, activation='leaky_relu'))\n",
    "pi_net.add(Dense(32, activation='leaky_relu'))\n",
    "pi_net.add(Dense(16))\n",
    "pi_net.add(Dense(4, activation='softmax')) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"  92.31% accuracy\\n    pi_net.add(Input(shape=(6,)))\\n    pi_net.add(Dense(32, activation='relu'))\\n    pi_net.add(Dropout(0.3))\\n    pi_net.add(Dense(64, activation='relu'))\\n    pi_net.add(Dropout(0.3))\\n    pi_net.add(Dense(32, activation='leaky_relu'))\\n    pi_net.add(Dense(64, activation='leaky_relu'))\\n    pi_net.add(Dropout(0.3))\\n    pi_net.add(Dense(128, activation='tanh'))\\n    pi_net.add(Dropout(0.5))\\n    pi_net.add(Dense(64, activation='relu'))\\n    pi_net.add(Dropout(0.5))\\n    pi_net.add(Dense(32, activation='gelu'))\\n    pi_net.add(Dense(4, activation='softmax')) \""
      ]
     },
     "execution_count": 733,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"  92.31% accuracy\n",
    "    pi_net.add(Input(shape=(6,)))\n",
    "    pi_net.add(Dense(32, activation='relu'))\n",
    "    pi_net.add(Dropout(0.3))\n",
    "    pi_net.add(Dense(64, activation='relu'))\n",
    "    pi_net.add(Dropout(0.3))\n",
    "    pi_net.add(Dense(32, activation='leaky_relu'))\n",
    "    pi_net.add(Dense(64, activation='leaky_relu'))\n",
    "    pi_net.add(Dropout(0.3))\n",
    "    pi_net.add(Dense(128, activation='tanh'))\n",
    "    pi_net.add(Dropout(0.5))\n",
    "    pi_net.add(Dense(64, activation='relu'))\n",
    "    pi_net.add(Dropout(0.5))\n",
    "    pi_net.add(Dense(32, activation='gelu'))\n",
    "    pi_net.add(Dense(4, activation='softmax')) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 93.75% accuracy\n",
    "\"\"\" pi_net.add(Dense(32, activation='leaky_relu'))\n",
    "pi_net.add(Dense(64, activation='leaky_relu'))\n",
    "pi_net.add(Dense(128, activation='tanh'))\n",
    "pi_net.add(Dropout(0.5))\n",
    "pi_net.add(Dense(64, activation='relu'))\n",
    "pi_net.add(Dropout(0.5))\n",
    "pi_net.add(Dense(32, activation='gelu'))\n",
    "pi_net.add(Dense(4, activation='softmax')) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # 94.23% accuracy\n",
    "    pi_net.add(Input(shape=(6,)))\n",
    "    pi_net.add(Dense(32, activation='silu'))\n",
    "    pi_net.add(Dense(64, activation='silu'))\n",
    "    pi_net.add(Dense(128, activation='tanh'))\n",
    "    pi_net.add(Dropout(0.25))\n",
    "    pi_net.add(Dense(256, activation='leaky_relu'))\n",
    "    pi_net.add(Dense(512, activation='silu'))\n",
    "    pi_net.add(Dropout(0.5))\n",
    "    pi_net.add(Dense(64, activation='leaky_relu'))\n",
    "    pi_net.add(Dropout(0.25))\n",
    "    pi_net.add(Dense(32, activation='gelu'))\n",
    "    pi_net.add(Dense(4, activation='softmax')) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  # 94.23% accuracy\n",
    "#     pi_net.add(Input(shape=(8,)))\n",
    "#     # pi_net.add(Dense(32, activation='silu'))\n",
    "#     pi_net.add(Dense(64, activation='silu'))\n",
    "#     pi_net.add(Dropout(0.35))\n",
    "#     pi_net.add(Dense(128, activation='tanh'))\n",
    "#     pi_net.add(Dropout(0.5))\n",
    "#     pi_net.add(Dense(64, activation='elu'))\n",
    "    \n",
    "#     # pi_net.add(Dense(32, activation='gelu'))\n",
    "#     pi_net.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 94.23% accuracy more consistent\n",
    "#     pi_net = Sequential()\n",
    "   \n",
    "#     # 94.23% accuracy\n",
    "#     pi_net.add(Input(shape=(6,)))\n",
    "#     # pi_net.add(Dense(32, activation='silu'))\n",
    "#     pi_net.add(Dense(64, activation='silu'))\n",
    "#     pi_net.add(Dropout(0.35))\n",
    "#     pi_net.add(Dense(128, activation='tanh'))\n",
    "#     pi_net.add(Dropout(0.5))\n",
    "#     pi_net.add(Dense(64, activation='elu'))\n",
    "    \n",
    "#     # pi_net.add(Dense(32, activation='gelu'))\n",
    "#     pi_net.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # # 94.23% accuracy\n",
    "    # pi_net.add(Input(shape=(6,)))\n",
    "    # pi_net.add(Dense(32, activation='silu'))\n",
    "    # pi_net.add(Dense(64, activation='tanh'))\n",
    "    # # pi_net.add(Dense(128, activation='leaky_relu'))\n",
    "    # pi_net.add(Dropout(0.5))\n",
    "    # pi_net.add(Dense(64, activation='elu'))\n",
    "    # pi_net.add(Dropout(0.5))\n",
    "    # pi_net.add(Dense(32, activation='gelu'))\n",
    "    # pi_net.add(Dense(4, activation='softmax'))\n",
    "    # if comp is True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95.83% accuracy\n",
    "\"\"\" pi_net.add(Input(shape=(6,))) \n",
    "pi_net.add(Dense(32, activation='leaky_relu'))\n",
    "pi_net.add(Dense(64, activation='leaky_relu'))\n",
    "pi_net.add(Dense(128, activation='tanh'))\n",
    "pi_net.add(Dropout(0.25))\n",
    "pi_net.add(Dense(256, activation='tanh'))\n",
    "pi_net.add(Dropout(0.5))\n",
    "pi_net.add(Dense(128, activation='tanh'))\n",
    "pi_net.add(Dropout(0.5))\n",
    "pi_net.add(Dense(64, activation='leaky_relu'))\n",
    "pi_net.add(Dropout(0.25))\n",
    "pi_net.add(Dense(32, activation='gelu'))\n",
    "pi_net.add(Dense(16, activation='leaky_relu'))\n",
    "pi_net.add(Dense(4, activation='softmax')) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 96.15% accuracy 2 wrong\n",
    "# pi_net.add(Dense(32, activation='silu'))\n",
    "# pi_net.add(Dense(64, activation='tanh'))\n",
    "# pi_net.add(Dense(128, activation='leaky_relu'))\n",
    "# pi_net.add(Dropout(0.5))\n",
    "# pi_net.add(Dense(64, activation='relu'))\n",
    "# pi_net.add(Dropout(0.5))\n",
    "# pi_net.add(Dense(32, activation='gelu'))\n",
    "# pi_net.add(Dense(4, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - categorical_accuracy: 0.9849 - loss: 0.0783  \n",
      "Neural Network Trainset: \n",
      "compile_metrics: 97.55%\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Testing Confusion Matrix: \n",
      "[[20  0  0  2]\n",
      " [ 0  7  0  0]\n",
      " [ 0  0  3  0]\n",
      " [ 1  0  0 19]]\n",
      "Testing Accuracy: 94.23%\n",
      "====================================\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93        22\n",
      "           1       1.00      1.00      1.00         7\n",
      "           2       1.00      1.00      1.00         3\n",
      "           3       0.90      0.95      0.93        20\n",
      "\n",
      "    accuracy                           0.94        52\n",
      "   macro avg       0.96      0.96      0.96        52\n",
      "weighted avg       0.94      0.94      0.94        52\n",
      "\n",
      "====================================\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'scoring' parameter of cross_validate must be a str among {'roc_auc_ovr', 'positive_likelihood_ratio', 'jaccard_micro', 'neg_brier_score', 'fowlkes_mallows_score', 'balanced_accuracy', 'neg_mean_squared_log_error', 'neg_mean_absolute_error', 'precision_weighted', 'explained_variance', 'jaccard_weighted', 'rand_score', 'jaccard', 'recall_macro', 'adjusted_rand_score', 'roc_auc_ovo_weighted', 'normalized_mutual_info_score', 'f1_samples', 'matthews_corrcoef', 'precision_macro', 'f1_macro', 'neg_root_mean_squared_log_error', 'neg_mean_poisson_deviance', 'precision', 'neg_negative_likelihood_ratio', 'precision_micro', 'roc_auc_ovr_weighted', 'f1_micro', 'recall', 'r2', 'neg_log_loss', 'jaccard_samples', 'f1', 'neg_mean_gamma_deviance', 'f1_weighted', 'precision_samples', 'top_k_accuracy', 'neg_median_absolute_error', 'neg_root_mean_squared_error', 'roc_auc', 'completeness_score', 'mutual_info_score', 'neg_mean_squared_error', 'recall_samples', 'homogeneity_score', 'accuracy', 'd2_absolute_error_score', 'adjusted_mutual_info_score', 'jaccard_macro', 'roc_auc_ovo', 'max_error', 'neg_mean_absolute_percentage_error', 'recall_micro', 'recall_weighted', 'average_precision', 'v_measure_score'}, a callable, an instance of 'list', an instance of 'tuple', an instance of 'dict' or None. Got 0.9423076923076923 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Perform cross-validation\u001b[39;00m\n\u001b[0;32m      7\u001b[0m kfold \u001b[38;5;241m=\u001b[39m KFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 8\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpi_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoded_y_pi_onehot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecall_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_test_pi_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_pred_pi_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweighted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# recall_score(y_true=Y_test_pi_labels, y_pred=Y_pred_pi_labels, average='weighted')\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCross-Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (+/- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults\u001b[38;5;241m.\u001b[39mstd()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jackg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:203\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    200\u001b[0m to_ignore \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    201\u001b[0m params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m to_ignore}\n\u001b[1;32m--> 203\u001b[0m \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameter_constraints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__qualname__\u001b[39;49m\n\u001b[0;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\jackg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'scoring' parameter of cross_validate must be a str among {'roc_auc_ovr', 'positive_likelihood_ratio', 'jaccard_micro', 'neg_brier_score', 'fowlkes_mallows_score', 'balanced_accuracy', 'neg_mean_squared_log_error', 'neg_mean_absolute_error', 'precision_weighted', 'explained_variance', 'jaccard_weighted', 'rand_score', 'jaccard', 'recall_macro', 'adjusted_rand_score', 'roc_auc_ovo_weighted', 'normalized_mutual_info_score', 'f1_samples', 'matthews_corrcoef', 'precision_macro', 'f1_macro', 'neg_root_mean_squared_log_error', 'neg_mean_poisson_deviance', 'precision', 'neg_negative_likelihood_ratio', 'precision_micro', 'roc_auc_ovr_weighted', 'f1_micro', 'recall', 'r2', 'neg_log_loss', 'jaccard_samples', 'f1', 'neg_mean_gamma_deviance', 'f1_weighted', 'precision_samples', 'top_k_accuracy', 'neg_median_absolute_error', 'neg_root_mean_squared_error', 'roc_auc', 'completeness_score', 'mutual_info_score', 'neg_mean_squared_error', 'recall_samples', 'homogeneity_score', 'accuracy', 'd2_absolute_error_score', 'adjusted_mutual_info_score', 'jaccard_macro', 'roc_auc_ovo', 'max_error', 'neg_mean_absolute_percentage_error', 'recall_micro', 'recall_weighted', 'average_precision', 'v_measure_score'}, a callable, an instance of 'list', an instance of 'tuple', an instance of 'dict' or None. Got 0.9423076923076923 instead."
     ]
    }
   ],
   "source": [
    "# Use KerasClassifier for scikit-learn compatibility\n",
    "net = create_pinet(False)\n",
    "Y_test_pi_labels, Y_pred_pi_labels = train_and_predict_pinets(model=net, opt=optimizers.Adam())\n",
    "model = KerasClassifier(model=net, epochs=200, batch_size=-1, verbose=1, metrics=['categorical_accuracy'])\n",
    "\n",
    "# Perform cross-validation\n",
    "kfold = KFold(n_splits=4, shuffle=True)\n",
    "results = cross_validate(model, X=pi_groups, y=encoded_y_pi_onehot, cv=kfold, scoring=recall_score(y_true=Y_test_pi_labels, y_pred=Y_pred_pi_labels, average='weighted'))\n",
    "# recall_score(y_true=Y_test_pi_labels, y_pred=Y_pred_pi_labels, average='weighted')\n",
    "print(f'Cross-Validation Accuracy: {results.mean():.2f} (+/- {results.std():.2f})')\n",
    "\n",
    "# train_and_predict_pinets(model=create_pinet(True), opt='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM (Classiefier)\n",
      "====================================\n",
      "Training Confusion Matrix: \n",
      "[[72  0  0  6]\n",
      " [17  8  0  0]\n",
      " [ 5  3  8  0]\n",
      " [10  0  0 75]]\n",
      "Training Accuracy: 79.90%\n",
      "====================================\n",
      "Testing Confusion Matrix: \n",
      "[[20  0  0  2]\n",
      " [ 6  1  0  0]\n",
      " [ 2  1  0  0]\n",
      " [ 4  0  0 16]]\n",
      "Testing Accuracy: 71.15%\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "clf = svm.SVC()\n",
    "train_and_predict_using_pimodel('SVM (Classiefier)', clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM (Classiefier) - RBF\n",
      "====================================\n",
      "Training Confusion Matrix: \n",
      "[[72  0  0  6]\n",
      " [17  8  0  0]\n",
      " [ 5  3  8  0]\n",
      " [10  0  0 75]]\n",
      "Training Accuracy: 79.90%\n",
      "====================================\n",
      "Testing Confusion Matrix: \n",
      "[[20  0  0  2]\n",
      " [ 6  1  0  0]\n",
      " [ 2  1  0  0]\n",
      " [ 4  0  0 16]]\n",
      "Testing Accuracy: 71.15%\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "rbf_clf = svm.SVC(kernel='rbf')\n",
    "train_and_predict_using_pimodel('SVM (Classiefier) - RBF', rbf_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM (Classiefier) - Poly\n",
      "====================================\n",
      "Training Confusion Matrix: \n",
      "[[48  0  0 32]\n",
      " [24  0  0  0]\n",
      " [ 2  0 14  0]\n",
      " [17  0  0 67]]\n",
      "Training Accuracy: 63.24%\n",
      "====================================\n",
      "Testing Confusion Matrix: \n",
      "[[10  0  0 10]\n",
      " [ 8  0  0  0]\n",
      " [ 1  0  2  0]\n",
      " [ 2  0  0 19]]\n",
      "Testing Accuracy: 59.62%\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "rbf_svc = svm.SVC(kernel='poly')\n",
    "train_and_predict_using_model('SVM (Classiefier) - Poly', rbf_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM (Classiefier) - Sigmoid\n",
      "====================================\n",
      "Training Confusion Matrix: \n",
      "[[42  0  0 38]\n",
      " [11  0  0 13]\n",
      " [ 0  0  0 16]\n",
      " [50  0  0 34]]\n",
      "Training Accuracy: 37.25%\n",
      "====================================\n",
      "Testing Confusion Matrix: \n",
      "[[12  0  0  8]\n",
      " [ 4  0  0  4]\n",
      " [ 0  0  0  3]\n",
      " [ 9  0  0 12]]\n",
      "Testing Accuracy: 46.15%\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "rbf_svc = svm.SVC(kernel='sigmoid')\n",
    "train_and_predict_using_model('SVM (Classiefier) - Sigmoid', rbf_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The classes, ['Broken or Intact Sheet', 'Crater', 'Lump', 'Splash'], are not in class_weight",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[555], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m wclf \u001b[38;5;241m=\u001b[39m svm\u001b[38;5;241m.\u001b[39mSVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, class_weight\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m2\u001b[39m})\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_and_predict_using_pimodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSVM (Classiefier) - Linear\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwclf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m, in \u001b[0;36mtrain_and_predict_using_pimodel\u001b[0;34m(model_name, model)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_and_predict_using_pimodel\u001b[39m(model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, model \u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscaled_X_train_pi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train_pi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     Y_pred_train_pi \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(scaled_X_train_pi)\n\u001b[1;32m      4\u001b[0m     cm_train_pi \u001b[38;5;241m=\u001b[39m confusion_matrix(Y_train_pi, Y_pred_train_pi)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/svm/_base.py:199\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    191\u001b[0m         X,\n\u001b[1;32m    192\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m         accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    197\u001b[0m     )\n\u001b[0;32m--> 199\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\n\u001b[1;32m    202\u001b[0m     [] \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[1;32m    203\u001b[0m )\n\u001b[1;32m    204\u001b[0m solver_type \u001b[38;5;241m=\u001b[39m LIBSVM_IMPL\u001b[38;5;241m.\u001b[39mindex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/svm/_base.py:740\u001b[0m, in \u001b[0;36mBaseSVC._validate_targets\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    738\u001b[0m check_classification_targets(y)\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28mcls\u001b[39m, y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_, return_inverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 740\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight_ \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_class_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mcls\u001b[39m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    743\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of classes has to be greater than one; got \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m class\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    744\u001b[0m         \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    745\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/class_weight.py:88\u001b[0m, in \u001b[0;36mcompute_class_weight\u001b[0;34m(class_weight, classes, y)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unweighted_classes \u001b[38;5;129;01mand\u001b[39;00m n_weighted_classes \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(class_weight):\n\u001b[1;32m     87\u001b[0m         unweighted_classes_user_friendly_str \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(unweighted_classes)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 88\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     89\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe classes, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munweighted_classes_user_friendly_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, are not in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m         )\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m weight\n",
      "\u001b[0;31mValueError\u001b[0m: The classes, ['Broken or Intact Sheet', 'Crater', 'Lump', 'Splash'], are not in class_weight"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
